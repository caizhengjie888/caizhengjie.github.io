[{"title":"Centos7下Flume安装配置与集成开发（超详细！！！）","url":"/posts/2867518176/","content":"## Centos7下Hadoop完全分布式集群Flume安装\n<!-- more -->\n - 电脑系统：macOS 10.15.4 \n - 虚拟机软件：Parallels Desktop14 \n - Hadoop各节点节点操作系统：CentOS 7\n - JDK版本：jdk1.8.0_162 \n - Flume版本：apache-flume-1.8.0-bin\n\nFlume的下载源地址：\n\n - [https://flume.apache.org/download.html](https://flume.apache.org/download.html)\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200627212830316.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\nbin文件是安装文件，src文件是flume源码，再做二次开发时候需要用到。\n\n - [有关flume的详细介绍请点击这里！](https://blog.csdn.net/weixin_45366499/article/details/106973208)\n\n## 第一步：安装软件\n### （1）上传文件\n将本机的安装包上传到虚拟机node1，上传方式：\n\n```bash\nscp 本机的文件绝对路径 caizhengjie@10.211.55.59:/opt/Hadoop\n```\n### （2）解压文件\n上传成功之后需要对文件赋予权限\n\n```bash\nchmod u+x apache-flume-1.8.0-bin.tar.gz\n```\n解压文件：\n\n```bash\ntar -zxvf apache-flume-1.8.0-bin.tar.gz\n```\n创建软链接：\n```bash\nln -s apache-flume-1.8.0-bin flume\n```\n## 第二步：配置环境变量\n```bash\nvim ~/.bashrc\n```\n然后添加以下内容，注意三台虚拟机都需要配置环境变量\n```bash\nexport FLUME_HOME=/opt/Hadoop/flume\nexport PATH=$FLUME_HOME/bin:$PATH\n```\n最后使之生效\n\n```bash\nsource ~/.bashrc\n```\n这里可以测试一下我们的flume有没有安装成功\n测试 `flume-ng version`\n当出现下面所示即表示安装成功\n\n```bash\n[caizhengjie@node3 Hadoop]$ flume-ng version\nFlume 1.8.0\nSource code repository: https://git-wip-us.apache.org/repos/asf/flume.git\nRevision: 99f591994468633fc6f8701c5fc53e0214b6da4f\nCompiled by denes on Fri Sep 15 14:58:00 CEST 2017\nFrom source with checksum fbb44c8c8fb63a49be0a59e27316833d\n```\n\n## 第三步：修改配置文件\n在修改配置文件之前我们需要了解我们的项目需求与设计。我们需要通过flume对应用服务器采集日志信息，最后分别送往kafka和HBase中。先看需求分析图\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200627213909394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n图中配置了两台分布式日志收集工具，一台日志合并预处理工具，在虚拟机中分别对应了node2，node3与node1。也就是说两台采集，一台汇总。这种场景模型也就是多Agent的复杂流组合，如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200627214404803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n### （1）更改文件名与分发文件\n**首先**进入node1中，将/opt/Hadoop/flume/conf/目录下的flume-conf.properties.template与flume-env.sh.template文件改名，原因是template表示模版文件。\n\n```bash\nmv flume-conf.properties.template flume-conf.properties\n```\n\n```bash\nmv flume-env.sh.template flume-env.sh\n```\n**然后**进入`flume-env.sh`添加JAVA_HOME\n\n```bash\nexport JAVA_HOME=/opt/Hadoop/jdk1.8.0_162\n```\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200627221631886.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n这里根据自己的路径做适当修改\n\n**最后**将flume与flume压缩文件移到node2，node3中\n```bash\nscp -r apache-flume-1.8.0-bin apache-flume-1.8.0-bin.tar.gz caizhengjie@node2:/opt/Hadoop/\nscp -r apache-flume-1.8.0-bin apache-flume-1.8.0-bin.tar.gz caizhengjie@node3:/opt/Hadoop/\n```\n分别在node2和node3上创建软连接\n\n```bash\nln -s apache-flume-1.8.0-bin flume\n```\n### （2）修改node2上的配置文件\n熟悉完项目需求分析之后，我们首先要配置node2最后配置node1。主要修改/opt/Hadoop/flume/conf/目录下的flume-conf.properties文件。我们需要将里面的内容删除，根据我们的要求重新配置，我这里参考的是官方文档\n\n - [https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html](https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html)\n\n**首先**配置好这三行，加在文件的最前面\n\n```bash\nagent2.sources = r1\nagent2.channels = c1\nagent2.sinks = k1\n```\n#### （1）source配置\n根据需求source我选择的类型是exec，可以根据官方文档来配置，查看官方文档流程：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200627225610132.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200627225808595.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n倒数第二行agent2.sources.r1.command = tail -F /opt/datas/weblogs.log，表示的是日志文件的路径，没有的话需要自己创建，其他的可以根据官网来，所以这里我的source配置是：\n\n```bash\n# source配置\nagent2.sources.r1.type = exec\nagent2.sources.r1.command = tail -F /opt/datas/weblogs.log\nagent2.sources.r1.channels = c1\n```\n#### （2）channel配置\n根据需求channel我选择的类型是memory，同样也可以参考官方文档：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200627230610456.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n所以这里我的channel配置是：\n\n```bash\n# channel配置\nagent2.channels.c1.type = memory\nagent2.channels.c1.capacity = 10000\nagent2.channels.c1.transactionCapacity = 10000\nagent2.channels.c1.keep-alive = 5\n```\n#### （3）sink配置\n根据需求sink我选择的类型是avro，同样也可以参考官方文档：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200627230833945.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n所以这里我的sink配置是：\n\n```bash\n# sink配置\nagent2.sinks.k1.type = avro\nagent2.sinks.k1.channel = c1\nagent2.sinks.k1.hostname = node1\nagent2.sinks.k1.port = 5555\n```\n\n综上我的flume-conf.properties文件总配置是：\n\n```bash\nagent2.sources = r1\nagent2.channels = c1\nagent2.sinks = k1\n\n# source配置\nagent2.sources.r1.type = exec\nagent2.sources.r1.command = tail -F /opt/datas/weblogs.log\nagent2.sources.r1.channels = c1\n\n# channel配置\nagent2.channels.c1.type = memory\nagent2.channels.c1.capacity = 10000\nagent2.channels.c1.transactionCapacity = 10000\nagent2.channels.c1.keep-alive = 5\n\n# sink配置\nagent2.sinks.k1.type = avro\nagent2.sinks.k1.channel = c1\nagent2.sinks.k1.hostname = node1\nagent2.sinks.k1.port = 5555\n```\n\n### （3）修改node3上的配置文件\n修改node3和node2一样，为了区别，只需将agent2改为了agent3，修改如下：\n\n```bash\nagent3.sources = r1\nagent3.channels = c1\nagent3.sinks = k1\n\n# source配置\nagent3.sources.r1.type = exec\nagent3.sources.r1.command = tail -F /opt/datas/weblogs.log\nagent3.sources.r1.channels = c1\n\n# channel配置\nagent3.channels.c1.type = memory\nagent3.channels.c1.capacity = 10000\nagent3.channels.c1.transactionCapacity = 10000\nagent3.channels.c1.keep-alive = 5\n\n# sink配置\nagent3.sinks.k1.type = avro\nagent3.sinks.k1.channel = c1\nagent3.sinks.k1.hostname = node1\nagent3.sinks.k1.port = 5555\n```\n\n### （4）修改node1上的配置文件（重点！）\n根据上面的需求，我们了解到node2和node3的flume采集了服务器的日志信息，然后汇总给了node1的flume，最后node1的flume分两个方向传送数据，一个传给kafka做实时计算，另一个传给HBase。我们下面需要进行配置主节点node1上面的flume，主节点的配置分为两个阶段，一个是基于Flume与HBase的集成配置，一个是基于Flume与Kafka的集成配置。配置的模型图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200629211328216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n#### （1）Flume与HBase的集成配置\n和node2的配置一样，主要修改/opt/Hadoop/flume/conf/目录下的flume-conf.properties文件。由上图可见，根据需求分析需要两个channel和两个sink。在文件的前面加上如下内容：\n\n```bash\nagent1.sources = r1\nagent1.channels = kafkaC hbaseC\nagent1.sinks = kafkaSink hbaseSink\n```\n##### （1）source配置\n根据需求source我选择的类型是avro，原因是node2和node3的sink类型就是avro，要保持类型的一致性，所以node1的source选择的类型是avro，可以根据官方文档来配置，查看官方文档流程：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200629211946656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n所以这里我node1的source配置是：\n\n```bash\n# source配置\nagent1.sources.r1.type = avro\nagent1.sources.r1.channels = hbaseC\nagent1.sources.r1.bind = node1\nagent1.sources.r1.port = 5555\nagent1.sources.r1.threads = 5\n```\n##### （2）channel配置\n根据需求channel我选择的类型是memory，同样也可以参考官方文档：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200629214808955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n所以这里我的channel配置是：\n\n```bash\n# channel配置\nagent1.channels.hbaseC.type = memory\nagent1.channels.hbaseC.capacity = 100000\nagent1.channels.hbaseC.transactionCapacity = 100000\nagent1.channels.hbaseC.keep-alive = 20\n```\n##### （3）sink配置\n根据需求sink我选择的类型是AsyncHBaseSink，同样也可以参考官方文档：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200629215630210.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n所以这里我的sink配置是：\n\n```bash\n# sink配置\nagent1.sinks.hbaseSink.type = asynchbase\nagent1.sinks.hbaseSink.table = weblogs\nagent1.sinks.hbaseSink.columnFamily = info\nagent1.sinks.hbaseSink.serializer = \nagent1.sinks.hbaseSink.channel = hbaseC\nagent1.sinks.hbaseSink.serializer.payloadColumn = datatime,userid,searchname,retorder,cliorder,cliurl\n```\n\n##### （4）对日志数据进行格式处理\n最后一行指的是hbase数据库的列名。根据相应的数据，我这里的用户日志数据格式，分为下面的六个部分组成：\n数据格式为：\n访问时间\\ t用户ID \\ t [查询词] \\ t该URL在返回结果中的排名\\ t用户点击的顺序号\\ t用户点击的URL\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200629221958586.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n我们需要将数据格式转换为用逗号隔开：\n那么我们需要执行两行命令\n\n```bash\n cat SogouQ.reduced |tr \"\\t\" \",\" > weblog2.log\n```\n解释：将源文件进行每一行转换，把tab换为逗号，最后生成一个新文件weblog2.log\n\n```bash\ncat weblog2.log |tr \" \" \",\" > weblog.log\n```\n解释：将weblog2.log文件进行每一行转换，把空格换为逗号，最后生成一个新文件weblog.log\n最后将weblog2.log文件删除即可\n可以查看一下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200629224259542.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n##### （5）对sinkHBase程序做二次开发\n根据项目需求，我们要通过flume将数据写入到hbase中，上面的步骤我们将日志数据的格式转换了，也就是说每一行数据代表hbase的六个列，那么我们就需要对sinkHBase程序做二次开发，首先要下载好源码，用IDEA打开/apache-flume-1.8.0-src/flume-ng-sinks/flume-ng-hbase-sink，最后找到`SimpleAsyncHbaseEventSerializer`类：\n没修改代码之前是这样：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200630214654344.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n首先将`SimpleAsyncHbaseEventSerializer`类复制并重新重命名`KfkAsyncHbaseEventSerializer`，在`getActions()`方法中修改：\n\n```java\n@Override\n  public List<PutRequest> getActions() {\n    List<PutRequest> actions = new ArrayList<PutRequest>();\n    if (payloadColumn != null) {\n      byte[] rowKey;\n      try {\n\n        // payloadColumn表示的是数据列名，因为每一行数据有六个列名，并且用逗号隔开\n        String[] columns =  String.valueOf(payloadColumn).split(\",\");\n        String[] values = String.valueOf(this.payload).split(\",\");\n\n        // 通过for循环将数据写入到actions\n        for (int i = 0;i<columns.length;i++){\n          byte[] colColumn = columns[i].getBytes();\n          byte[] colValue = values[i].getBytes(Charsets.UTF_8);\n\n          if(colColumn.length != colValue.length) {\n            break;\n          }\n          if (colValue.length < 3){\n            break;\n          }\n\n          String datetime = values[0].toString();\n          String userid = values[1].toString();\n          rowKey = SimpleRowKeyGenerator.getKfkRowKey(userid,datetime);\n          PutRequest putRequest =  new PutRequest(table, rowKey, cf,\n                  colColumn, colValue);\n          actions.add(putRequest);\n        }\n\n      } catch (Exception e) {\n        throw new FlumeException(\"Could not get row key!\", e);\n      }\n    }\n    return actions;\n  }\n```\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200630215016408.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n修改完成之后需要打成jar包上传到项目当中/opt/Hadoop/flume/lib\n[打成jar包的博客参考这里！](https://blog.csdn.net/weixin_45366499/article/details/104851893)\n打出来之后的jar包是flume-ng-hbase-sink.jar，需要将项目中flume-ng-hbase-sink-1.8.0.jar这个包删除，然后重新命名。\n\n**综上**我的flume-conf.properties文件总配置是：\n\n```bash\nagent1.sources = r1\nagent1.channels = kafkaC hbaseC\nagent1.sinks = kafkaSink hbaseSink\n\n# source配置\nagent1.sources.r1.type = avro\nagent1.sources.r1.channels = hbaseC\nagent1.sources.r1.bind = node1\nagent1.sources.r1.port = 5555\nagent1.sources.r1.threads = 5\n\n# channel配置\nagent1.channels.hbaseC.type = memory\nagent1.channels.hbaseC.capacity = 100000\nagent1.channels.hbaseC.transactionCapacity = 100000\nagent1.channels.hbaseC.keep-alive = 20\n\n# sink配置\nagent1.sinks.hbaseSink.type = asynchbase\nagent1.sinks.hbaseSink.table = weblogs\nagent1.sinks.hbaseSink.columnFamily = info\nagent1.sinks.hbaseSink.serializer = org.apache.flume.sink.hbase.KfkAsyncHbaseEventSerializer\nagent1.sinks.hbaseSink.channel = hbaseC\nagent1.sinks.hbaseSink.serializer.payloadColumn = datatime,userid,searchname,retorder,cliorder,cliurl\n```\n#### （2）Flume与Kafka的集成配置\n##### （1）channel配置\n在flume与kafka集成配置的时候与前面步骤相似，这里我就直接放上配置文件：\n\n```java\n# channel配置\nagent1.channels.hbaseC.type = memory\nagent1.channels.hbaseC.capacity = 100000\nagent1.channels.hbaseC.transactionCapacity = 100000\nagent1.channels.hbaseC.keep-alive = 20\n```\n\n##### （2）sink配置\n```java\n# sink配置\nagent1.sinks.kafkaSink.channel = kafkaC\nagent1.sinks.kafkaSink.type = org.apache.flume.sink.kafka.KafkaSink\nagent1.sinks.kafkaSink.topic = test\nagent1.sinks.kafkaSink.brokerList = ndoe1:9092,ndoe2:9092,ndoe3:9092\nagent1.sinks.kafkaSink.zookeeperConnect = node1:2181,node2:2181,node3:2181\nagent1.sinks.kafkaSink.requiredAcks = 1\nagent1.sinks.kafkaSink.batchSize = 1\nagent1.sinks.kafkaSink.serializer.class = kafka.serializer.StringEncoder\n```\n\n---\n这里node1的flume-conf.properties文件总配置是：\n\n```java\nagent1.sources = r1\nagent1.channels = kafkaC hbaseC\nagent1.sinks = kafkaSink hbaseSink\n\n# ************************flume + hbase***************************\n\n# source配置\nagent1.sources.r1.type = avro\nagent1.sources.r1.channels = hbaseC\nagent1.sources.r1.bind = node1\nagent1.sources.r1.port = 5555\nagent1.sources.r1.threads = 5\n\n# channel配置\nagent1.channels.hbaseC.type = memory\nagent1.channels.hbaseC.capacity = 100000\nagent1.channels.hbaseC.transactionCapacity = 100000\nagent1.channels.hbaseC.keep-alive = 20\n\n# sink配置\nagent1.sinks.hbaseSink.type = asynchbase\nagent1.sinks.hbaseSink.table = weblogs\nagent1.sinks.hbaseSink.columnFamily = info\nagent1.sinks.hbaseSink.serializer = org.apache.flume.sink.hbase.KfkAsyncHbaseEventSerializer\nagent1.sinks.hbaseSink.channel = hbaseC\nagent1.sinks.hbaseSink.serializer.payloadColumn = datatime,userid,searchname,retorder,cliorder,cliurl\n\n# ************************flume + kafka***************************\n\n# channel配置\nagent1.channels.hbaseC.type = memory\nagent1.channels.hbaseC.capacity = 100000\nagent1.channels.hbaseC.transactionCapacity = 100000\nagent1.channels.hbaseC.keep-alive = 20\n\n# sink配置\nagent1.sinks.kafkaSink.channel = kafkaC\nagent1.sinks.kafkaSink.type = org.apache.flume.sink.kafka.KafkaSink\nagent1.sinks.kafkaSink.topic = test\nagent1.sinks.kafkaSink.brokerList = ndoe1:9092,ndoe2:9092,ndoe3:9092\nagent1.sinks.kafkaSink.zookeeperConnect = node1:2181,node2:2181,node3:2181\nagent1.sinks.kafkaSink.requiredAcks = 1\nagent1.sinks.kafkaSink.batchSize = 1\nagent1.sinks.kafkaSink.serializer.class = kafka.serializer.StringEncoder\n```\n----\n以上内容仅供参考学习，如有侵权请联系我删除！\n如果这篇文章对您有帮助，左下角的大拇指就是对博主最大的鼓励。\n您的鼓励就是博主最大的动力！","tags":["Flume"],"categories":["Flume学习指南"]},{"title":"Flume架构原理详解","url":"/posts/1471002198/","content":"## 一、概念理解\n<!-- more -->\n### （1）Flume简介\nApache Flume是一个分布式，可靠且可用的系统，用于有效地收集，聚合大量日志数据并将其从许多不同的源移动到集中式数据存储中。Apache Flume的使用不仅限于日志数据聚合。由于数据源是可定制的，因此Flume可用于传输大量事件数据，包括但不限于网络流量数据，社交媒体生成的数据，电子邮件消息以及几乎所有可能的数据源。Apache Flume是Apache Software Foundation的顶级项目。\n### （2）Flume特点\n#### （1）可靠性\n当节点出现故障时，日志能够被传送到其他节点上而不会丢失。Flume提供了三种级别的可靠性保障，从强到弱依次分别为：\n\n - **end-to-end**：收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送\n - **Store on failure**：这也是scribe采用的策略，当数据接收方crash时，将数据写到本地，待恢复后，继续发送\n - **Best effort**：数据发送到接收方后，不会进行确认\n\n#### （2）可恢复性\n事件在通道中上演，该通道管理从故障中恢复。Flume支持持久的文件通道，该通道由本地文件系统支持。还有一个内存通道可以将事件简单地存储在内存队列中，这样速度更快，但是当代理进程死亡时，仍保留在内存通道中的任何事件都无法恢复。\n#### （3）可扩展性\nFlume采用了三层架构，分别为agent，collector和storage，每一层均可以水平扩展。\n其中，所有agent和collector由master统一管理，这使得系统容易监控和维护，且master允许有多个（使用ZooKeeper进行管理和负载均衡），这就避免了单点故障问题。\n#### （4）可管理性\n\n - 所有agent和colletor由master统一管理，这使得系统便于维护。\n - 多master情况，Flume利用ZooKeeper和gossip，保证动态配置数据的一致性。\n - 用户可以在master上查看各个数据源或者数据流执行情况，且可以对各个数据源配置和动态加载。\n - Flume提供了web 和shell script command两种形式对数据流进行管理。\n\n#### （5）功能可扩展性\n\n - 用户可以根据需要添加自己的agent，collector或者storage。\n - Flume自带了很多组件，包括各种agent（file， syslog等），collector和storage（file，HDFS等）。\n\n## 二、Flume中核心架构组件\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200626193917435.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n结合上图，Flume的一些核心组件\n\n - **Web Server**：数据产生的源头。\n - **Agent**：Flume的核心就是Agent 。Agent是一个Java进程，包含组件Source、 Channel、 Sink，且运行在日志收集端，通过Agent接收日志，然后暂存起来，再发送到目的地。（Agent使用JVM 运行Flume。每台机器运行多个agent，但是在一个agent中只能包含一个source。）\n - **Source**：Agent核心组件之一，Source（源）用于从Web Server收集数据，然后发送到Channel（通道）。\n - **Channel**：Agent核心组件之一，Channel（通道）可以用来从Source接收数据，然后发送到Sink，Channel存放临时数据，有点类似队列一样。\n - **Sink**：Agent核心组件之一，Sink（接收器）用来把数据发送的目标地点，如上图放到HDFS中。\n - **Event**：整个数据传输过程中，流动的对象都是实现了org.apache.flume.Event接口的对象。Event也是事务保证的级别。\n - **Flow**：Event从源点到达目的点的迁移的抽象\n\n### （1）Agent\nFlume 运行的核心是 Agent。Flume以agent为最小的独立运行单位。一个agent就是一个JVM。它是一个完整的数据收集工具，含有三个核心组件，分别是source、 channel、 sink。通过这些组件， Event 可以从一个地方流向另一个地方。\n### （2）source\nSource是数据的收集端，负责将数据捕获后进行特殊的格式化，将数据封装到事件（event） 里，然后将事件推入Channel中。 Flume提供了很多内置的Source， 支持 Avro， log4j， syslog 和 http post(body为json格式)。可以让应用程序同已有的Source直接打交道，如AvroSource，SyslogTcpSource。 如果内置的Source无法满足需要， Flume还支持自定义Source。\n\nsource类型：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200626195311695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n### （2）Channel\nChannel是连接Source和Sink的组件，大家可以将它看做一个数据的缓冲区（数据队列），它可以将事件暂存到内存中也可以持久化到本地磁盘上， 直到Sink处理完该事件。\n\nChannel类型：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200626195515185.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n### （3）Sink\nSink从Channel中取出事件，然后将数据发到别处，可以向文件系统、数据库、 hadoop存数据， 也可以是其他agent的Source。在日志数据较少时，可以将数据存储在文件系统中，并且设定一定的时间间隔保存数据。\n\nSink类型：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200626195620606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n## 三、Flume拦截器、数据流以及可靠性\n### （1）Flume拦截器\n当我们需要对数据进行过滤时，除了我们在Source、 Channel和Sink进行代码修改之外， Flume为我们提供了拦截器，拦截器也是chain形式的。拦截器的位置在Source和Channel之间，当我们为Source指定拦截器后，我们在拦截器中会得到event，根据需求我们可以对event进行保留还是抛弃，抛弃的数据不会进入Channel中。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200626201721277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n### （2）Flume数据流\n\n - Flume 的核心是把数据从数据源收集过来，再送到目的地。为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据。\n - Flume 传输的数据的基本单位是 Event，如果是文本文件，通常是一行记录，这也是事务的基本单位。 Event 从 Source，流向 Channel，再到 Sink，本身为一个 byte 数组，并可携带 headers 信息。 Event 代表着一个数据流的最小完整单元，从外部数据源来，向外部的目的地去。\n\n值得注意的是，Flume提供了大量内置的Source、Channel和Sink类型。不同类型的Source,Channel和Sink可以自由组合。组合方式基于用户设置的配置文件，非常灵活。比如：Channel可以把事件暂存在内存里，也可以持久化到本地硬盘上。Sink可以把日志写入HDFS, HBase，甚至是另外一个Source等等。Flume支持用户建立多级流，也就是说，多个agent可以协同工作，并且支持Fan-in、Fan-out、Contextual Routing、Backup Routes，这也正是Flume强大之处。如图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200626202225981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n### （3）Flume可靠性\nFlume 使用事务性的方式保证传送Event整个过程的可靠性。 Sink 必须在Event 被存入 Channel 后，或者，已经被传达到下一站agent里，又或者，已经被存入外部数据目的地之后，才能把 Event 从 Channel 中 remove 掉。这样数据流里的 event 无论是在一个 agent 里还是多个 agent 之间流转，都能保证可靠，因为以上的事务保证了 event 会被成功存储起来。比如 Flume支持在本地保存一份文件 channel 作为备份，而memory channel 将event存在内存 queue 里，速度快，但丢失的话无法恢复。\n\n## 四、Flume使用场景\n### （1）多个agent顺序连接\n可以将多个Agent顺序连接起来，将最初的数据源经过收集，存储到最终的存储系统中。这是最简单的情况，一般情况下，应该控制这种顺序连接的Agent 的数量，因为数据流经的路径变长了，如果不考虑failover的话，出现故障将影响整个Flow上的Agent收集服务。 \n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200626202708491.png)\n这个例子里面为了能让数据流在多个Agent之间传输，前一个Agent的sink必须和后一个Agent的source都需要设置为avro类型并且指向相同的hostname（或者IP）和端口。\n### （2）多Agent的复杂流\n这种情况应用的场景比较多，比如要收集Web网站的用户行为日志， Web网站为了可用性使用的负载集群模式，每个节点都产生用户行为日志，可以为每个节点都配置一个Agent来单独收集日志数据，然后多个Agent将数据最终汇聚到一个用来存储数据存储系统，如HDFS上。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200626202849243.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n可以通过使用 Avro Sink 配置多个第一层 Agent（Agent1、Agent2、Agent3），所有第一层Agent的Sink都指向下一级同一个Agent（Agent4）的 Avro Source上（同样你也可以使用 thrift 协议的 Source 和 Sink 来代替）。Agent4 上的 Source 将 Event 合并到一个 channel 中，该 channel中的Event最终由HDFS Sink 消费发送到最终目的地。\n### （3）多路复用流\nFlume支持多路复用数据流到一个或多个目的地。这是通过使用一个流的[多路复用器]（multiplexer ）来实现的，它可以 复制 或者 选择 数据流到一个或多个channel上。\n\n很容易理解， **复制** 就是每个channel的数据都是完全一样的，每一个channel上都有完整的数据流集合。 **选择** 就是通过自定义一个分配机制，把数据流拆分到多个channel上。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200626203119109.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n上图的例子展示了从Agent foo扇出流到多个channel中。这种扇出的机制可以是复制或者选择。当配置为复制的时候，每个Event都被发送到3个channel上。当配置为选择的时候，当Event的某个属性与配置的值相匹配时会被发送到对应的channel。\n\n例如Event的属性txnType是customer时，Event被发送到channel1和channel3，如果txnType的值是vendor时，Event被发送到channel2，其他值一律发送到channel3，这种规则是可以通过配置来实现的。\n\n参考文章：\n\n - [https://flume.liyifeng.org/#flume-sources](https://flume.liyifeng.org/#flume-sources)（中文官方文档）\n - [https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#architecture](https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#architecture)（官方文档）\n - [https://www.cnblogs.com/zhangyinhua/p/7803486.html](https://www.cnblogs.com/zhangyinhua/p/7803486.html)（推荐阅读）\n - [https://yq.aliyun.com/articles/369857](https://yq.aliyun.com/articles/369857)\n - [https://www.iteye.com/blog/qianshangding-2259404](https://www.iteye.com/blog/qianshangding-2259404)\n\n---\n以上内容仅供参考学习，如有侵权请联系我删除！\n如果这篇文章对您有帮助，左下角的大拇指就是对博主最大的鼓励。\n您的鼓励就是博主最大的动力！\n","tags":["Flume"],"categories":["Flume学习指南"]},{"title":"Centos7下Kafka安装与配置","url":"/posts/2222169886/","content":"## Centos7下Hadoop完全分布式集群Kafka安装\n<!-- more -->\n - 电脑系统：macOS 10.15.4 \n - 虚拟机软件：Parallels Desktop14 \n - Hadoop各节点节点操作系统：CentOS 7\n - JDK版本：jdk1.8.0_162 \n - Kafka版本：kafka_2.11-2.1.1\n\nKafka的下载源地址：\n\n - [https://archive.apache.org/dist/kafka/2.1.1/](https://archive.apache.org/dist/kafka/2.1.1/)\n\n[想详细了解kafka原理的点击这里！](https://blog.csdn.net/weixin_45366499/article/details/106943229)\n## 第一步：安装软件\n### （1）上传文件\n将本机的安装包上传到虚拟机node1，上传方式：\n\n```bash\nscp 本机的文件绝对路径 caizhengjie@10.211.55.59:/opt/Hadoop\n```\n### （2）解压文件\n上传成功之后需要对文件赋予权限\n\n```bash\nchmod u+x kafka_2.11-2.1.1.tar.gz\n```\n解压文件：\n\n```bash\ntar -zxvf kafka_2.11-2.1.1.tar.gz\n```\n创建软链接：\n```bash\nln -s kafka_2.11-2.1.1 kafka\n```\n\n## 第二步：配置环境变量\n声明：kafka不同的版本对应的配置文件可能不同，命令也不一样，所以在参考之前需要核对好自己的版本号，以免产生误会。\n```bash\nvim ~/.bashrc\n```\n然后添加以下内容，注意三台虚拟机都需要配置环境变量\n```bash\nexport KAFKA_HOME=/opt/Hadoop/kafka\nexport PATH=${KAFKA_HOME}/bin:$PATH\n```\n最后使之生效\n\n```bash\nsource ~/.bashrc\n```\n## 第三步：修改配置文件\nkafka中修改配置文件主要是`/opt/Hadoop/kafka/config`中的`server.properties`\n\n**第一个地方：**\n```bash\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id=1\n```\n在node1中修改编号为1，node2中修改编号为2，node3中修改编号为3，以此类推\n\n**第二个地方：**\n```bash\n# The address the socket server listens on. It will get the value returned from \n# java.net.InetAddress.getCanonicalHostName() if not configured.\n#   FORMAT:\n#     listeners = listener_name://host_name:port\n#   EXAMPLE:\n#     listeners = PLAINTEXT://your.host.name:9092\nlisteners=PLAINTEXT://node1:9092\nport=9092\n```\n这里根据自己的所在的主机名修改\n\n**第三个地方：**\n\n```bash\n# Hostname and port the broker will advertise to producers and consumers. If not set, \n# it uses the value for \"listeners\" if configured.  Otherwise, it will use the value\n# returned from java.net.InetAddress.getCanonicalHostName().\nadvertised.listeners=PLAINTEXT://node1:9092\n```\n这里根据自己的所在的主机名修改\n\n**第四个地方：**\n\n```bash\n# A comma separated list of directories under which to store log files\nlog.dirs=/opt/Hadoop/kafka/kafka-logs\n```\n这里需要在`/opt/Hadoop/kafka/`目录下创建一个`kafka-logs`文件夹\n\n```bash\nmkdir kafka-logs\n```\n\n**第五个地方：**\n```bash\n# Zookeeper connection string (see zookeeper docs for details).\n# This is a comma separated host:port pairs, each corresponding to a zk\n# server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\".\n# You can also append an optional chroot string to the urls to specify the\n# root directory for all kafka znodes.\nzookeeper.connect=node1:2181,node2:2181,node3:2181\n```\nzookeeper这里需要按照这里的格式写\n\n## 第四步：分发配置文件\n将node1的kafka_2.11-2.1.1和kafka_2.11-2.1.1.tar.gz文件分发到node2和node3上：\n```bash\nscp -r kafka_2.11-2.1.1 kafka_2.11-2.1.1.tar.gz caizhengjie@node2:/opt/Hadoop/\nscp -r kafka_2.11-2.1.1 kafka_2.11-2.1.1.tar.gz caizhengjie@node3:/opt/Hadoop/\n```\n分别在node2和node3上创建软连接\n\n```bash\nln -s kafka_2.11-2.1.1.tar.gz kafka\n```\n上传完之后要对node2，node3进行配置文件的修改，参考第三步\n## 第五步：启动与检测\nkafka的启动方式，需要进入到bin的上一级目录，也就是`/opt/Hadoop/kafka`\n\n```bash\nbin/kafka-server-start.sh -daemon config/server.properties\n```\n通过输入jps来查看进程\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200624230149612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200624230149569.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200624230149565.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n出现kafka则表示安装成功\n\n## 第六步：对Kafka进行消息测试\n在测试之前，需要配置一下`producer.properties`文件，意思是配置生产者。\n我在每一台机的/opt/Hadoop/kafka/config/producer.properties文件中修改\n```bash\n# list of brokers used for bootstrapping knowledge about the rest of the cluster\n# format: host1:port1,host2:port2 ...\nbootstrap.servers=node1:9092,node2:9092,node3:9092\n```\n### （1）启动服务\n首先要启动zookeeper，再启动kafka，三台要同时启动\n启动zookeeper：\n\n```bash\nzkServer.sh start\n```\n启动kafka\n在前台启动kafka，注意查看打印在桌面的日志，有无报错信息\n\n```bash\nbin/kafka-server-start.sh config/server.properties\n```\n如果没有报错信息，启动正常，那么就可以在后台启动了\n```bash\nbin/kafka-server-start.sh -daemon config/server.properties\n```\n### （2）创建topic\n创建一个分区和一个副本的“test”的topic\n\n```bash\nbin/kafka-topics.sh --create --zookeeper node1:2181,node2:2181,node3:2181 --replication-factor 1 --partitions 1 --topic test\n```\n这里有的朋友会问到，topic在哪里可以看到？那么我这里解释一下\n首先进入到`/opt/Hadoop/zookeeper/bin`目录下\n\n```bash\ncd /opt/Hadoop/zookeeper/bin\n```\n然后执行`zkCli.sh`服务，会看到下图的页面\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200625001026684.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n然后输入`ls /`，则下面就看到brokers，输入`ls /brokers`，则会看到\n\n```bash\n[zk: localhost:2181(CONNECTED) 2] ls /brokers\n[ids, topics, seqid]\n```\n那么刚才所创建的test就在topics下，再次输入`ls /brokers/topics`，就会看到test\n\n```bash\n[zk: localhost:2181(CONNECTED) 3] ls /brokers/topics\n[protest, test, __consumer_offsets]\n```\n输入quit则退出\n\n还有一种方法，可以通过下面的命令查看\n\n```bash\nbin/kafka-topics.sh --list --zookeeper localhost:2181\n```\n\n```bash\n[caizhengjie@node1 kafka]$ bin/kafka-topics.sh --list --zookeeper node1:2181\n__consumer_offsets\nprotest\ntest\n```\n### （3）生产者发送消息\nKafka带有一个命令行客户端，它将从文件或标准输入中获取输入，并将其作为消息发送到Kafka集群。默认情况下，每行将作为单独的消息发送。运行生产者，然后在控制台中键入一些消息以发送到服务器。\n\n```bash\nbin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\n```\n\n```bash\n[caizhengjie@node1 kafka]$ bin/kafka-console-producer.sh --broker-list node1:9092,node2:9092,node3:9092, --topic test\n>java\n>python\n>php\n>hadoop\n>linux\n>\n```\n### （4）消费者接收消息\nKafka还有一个命令行使用者，它将消息转储到标准输出。\n\n```bash\nbin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning\n```\n在node2上操作：\n\n```bash\nbin/kafka-console-consumer.sh --bootstrap-server node1:9092,node2:9092,node3:9092 --from-beginning --topic test\njava\npython\nphp\nhadoop\nlinux\n\n```\n参考文章：\n官网文档\n\n - [https://kafka.apache.org/21/documentation.html#quickstart](https://kafka.apache.org/21/documentation.html#quickstart)\n\n---\n以上内容仅供参考学习，如有侵权请联系我删除！\n如果这篇文章对您有帮助，左下角的大拇指就是对博主最大的鼓励。\n您的鼓励就是博主最大的动力！\n","tags":["Kafka"],"categories":["Kafka学习指南"]},{"title":"YARN基本架构原理详解","url":"/posts/3921886647/","content":"## 一、概念理解\n<!-- more -->\nApache Hadoop YARN（Yet Another Resource Negotiator,另一种资源协调者）是一种新的Hadoop资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来的巨大的好处。\n\n**YARN产生背景：**\na）JobTracker单点故障\nb）JobTracker承受的访问压力大，影响系统的扩展性\nc）不支持MapReduce之外的计算框架，比如storm、spark、flink\n\n**YARN的优势：**\n资源的统一管理和调度： \n\n - 集群中所有节点的资源(内存、CPU、磁盘、网络等)抽象为Container。计算框架需要资源进行运算任务时需要向YARN申请Container， YARN按照特定的策略对资源进行调度进行Container的分配。\n \n资源隔离： \n - YARN使用了轻量级资源隔离机制Cgroups进行资源隔离以避免相互干扰，一旦Container使用的资源量超过事先定义的上限值，就将其杀死。\n## 二、YARN的架构\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200622105442387.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n从YARN的架构来看，它由ResourceManager、NodeManager、JobHistoryServer、Containers、Application Master、job、Task、Client组成。\n\n组件的主要功能介绍：\n**（1）ResourceManager：**\n - 处理客户端请求\n - 启动/监控ApplicationMaster\n - 监控NodeManager\n - 资源分配与调度\n \n**（2）ApplicationMaster：**\n - 为应用程序申请资源，并分配给内部任务\n - 任务调度、监控与容错\n\n**（3）NodeManager：**\n\n - 单个节点上的资源管理\n - 处理来自ResourceManger的命令\n - 处理来自ApplicationMaster的命令\n\n**（4）Container：**\n - 对资源抽象和封装，目的是为了让每个应用程序对应的任务完成执行\n\n**（5）JobHistoryServer：**\n\n - 负责查询job运行进度及元数据管理。\n\n**（6）job：**\n\n - 是需要执行的一个工作单元：它包括输入数据、MapReduce程序和配置信息。job也可以叫作Application。\n\n**（7）task：**\n\n - 一个具体做Mapper或Reducer的独立的工作单元。task运行在NodeManager的Container中。\n\n**（8）Client：**\n\n - 一个提交给ResourceManager的一个Application程序。\n\n\n### （1）ResourceManager\nResourceManager（RM）是一个全局的资源管理器，负责整个系统的资源管理和分配，主要包括两个组件，即调度器（Scheduler）和应用程序管理器（Applications Manager）。\n\n调度器接收来自ApplicationMaster的应用程序资源请求，把集群中的资源以“容器”的形式分配给提出申请的应用程序，容器的选择通常会考虑应用程序所要处理的数据的位置，进行就近选择，从而实现“计算向数据靠拢”。\n\n容器（Container）作为动态资源分配单位，每个容器中都封装了一定数量的CPU、内存、磁盘等资源，从而限定每个应用程序可以使用的资源量。\n\n调度器被设计成是一个可插拔的组件，YARN不仅自身提供了许多种直接可用的调度器，也允许用户根据自己的需求重新设计调度器。\n\n应用程序管理器（Applications Manager）负责系统中所有应用程序的管理工作，主要包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动等。\n\n### （2）ApplicationMaster\nResourceManager接收用户提交的作业，按照作业的上下文信息以及从NodeManager收集来的容器状态信息，启动调度过程，为用户作业启动一个ApplicationMaster。\n\nApplicationMaster的主要功能是：\n\n（1）当用户作业提交时，ApplicationMaster与ResourceManager协商获取资源，ResourceManager会以容器的形式为ApplicationMaster分配资源；\n\n（2）把获得的资源进一步分配给内部的各个任务（Map任务或Reduce任务），实现资源的“二次分配”；\n\n（3）与NodeManager保持交互通信进行应用程序的启动、运行、监控和停止，监控申请到的资源的使用情况，对所有任务的执行进度和状态进行监控，并在任务发生失败时执行失败恢复（即重新申请资源重启任务）；\n\n（4）定时向ResourceManager发送“心跳”消息，报告资源的使用情况和应用的进度信息；\n\n（5）当作业完成时，ApplicationMaster向ResourceManager注销容器，执行周期完成。\n\n### （3）NodeManager\nNodeManager是驻留在一个YARN集群中的每个节点上的代理，主要负责：\n\n - 容器生命周期管理。 \n - 监控每个容器的资源（CPU、内存等）使用情况。 \n - 跟踪节点健康状况。\n - 以“心跳”的方式与ResourceManager保持通信。 \n - 向ResourceManager汇报作业的资源使用情况和每个容器的运行状态。\n - 接收来自ApplicationMaster的启动/停止容器的各种请求 。\n\n需要说明的是，NodeManager主要负责管理抽象的容器，只处理与容器相关的事情，而不具体负责每个任务（Map任务或Reduce任务）自身状态的管理，因为这些管理工作是由ApplicationMaster完成的，ApplicationMaster会通过不断与NodeManager通信来掌握各个任务的执行状态。\n\n## 三、Yarn基本流程\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200622113107799.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n\n 1. 用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序\n 2. ResourceManager为该应用程序分配第一个Container，并与对应的Node-Manager通信，要求它在这个Container中启动应用程序的ApplicationMaster\n 3. ApplicationMaster首先向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7\n 4. ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源\n 5. 一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务\n 6. NodeManager为任务设置好运行环境(包括环境变量、JAR包、二进制程序等)后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务\n 7. 各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过RPC向ApplicationMaster查询应用程序的当前运行状态\n 8. 应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己\n\n## 四、一个job运行处理的整体流程\n用户向YARN中提交作业，其中包括Application Master启动、Application Master的命令及用户程序等；ResourceManager为作业分配第一个Container，并与对应的NodeManager通信，要求它在这个Container中启动该作业的Application Master；Application Master首先向ResourceManager注册，这样用户可以直接通过ResourceManager查询作业的运行状态，然后它将为各个任务申请资源并监控任务的运行状态，直到任务结束。Application通过RPC请求想ResourceManager申请和领取资源。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200622113506433.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n然后ApplicationMaster要求指定的NodeManager节点启动任务。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200622113546682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n启动之后，去干ResoucrceManager指定的Map task。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200622113625308.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n等Map task干完之后，通知Application Master。然后Application Master去告知Resource Manager。接下来Resource Manager分配新的资源给Application Master，让它找人去干其他的活。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200622113656159.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n接下来Application Master通知NodeManager启动新的Container准备干新的活，该活的输入是Map task的输出。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200622113732156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n开始干Reduce task任务。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020062211380170.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n等各个节点的Reduce task都干好了，将干活的NodeManager的任务结果进行同步。做最后的Reduce任务。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200622113829506.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n等计算完了，最后将最终的结果输出到HDFS。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200622113911221.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n任务完成！\n\n## 五、Yarn调度器Scheduler\n理想情况下，我们应用对 Yarn 资源的请求应该立刻得到满足，但现实情况资源往往是有限的，特别是在一个很繁忙的集群，一个应用资源的请求经常需要等待一段时间才能的到相应的资源。在Yarn中，负责给应用分配资源的就是Scheduler。其实调度本身就是一个难题，很难找到一个完美的策略可以解决所有的应用场景。为此Yarn提供了多种调度器和可配置的策略供我们选择。在 Yarn 中有三种调度器可以选择：**FIFO Scheduler ，Capacity Scheduler，Fair Scheduler。**\n\n**三种调度器基本原理：**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200622114200742.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n**FIFO Scheduler（先进先出调度器）：** 把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推\n\n工作方法：\n\n - 单队列\n - 先进先出原则\n\n**Capacity Scheduler（容器调度器）：** 调度器允许多个组织共享整个集群，每个组织可以获得集群的一部分计算能力。通过为每个组织分配专门的队列，然后再为每个队列分配一定的集群资源，这样整个集群就可以通过设置多个队列的方式给多个组织提供服务了。除此之外，队列内部又可以垂直划分，这样一个组织内部的多个成员就可以共享这个队列资源了，在一个队列内部，资源的调度是采用的是先进先出(FIFO)策略。\n\n工作方法：\n\n - 多队列\n - 资源使用量最小、优先级高的先执行\n - 在多用户的情况下，可以最大化集群的吞吐量和利用率\n\n**Fair Scheduler（公平调度器）：** 针对不同的应用（也可以为用户或用户组），每个应用属于一个队列，主旨是让每个应用分配的资源大体相当。（当然可以设置权重），若是只有一个应用，那集群所有资源都是他的。和 Capacity的区别是不需要预留资源 。适用情况：共享大集群、队列之间有较大差别。\n\n工作方法：\n\n - 多队列\n - 公平调度，所有任务具有相同的资源\n\n## 六、YARN的HA架构\n\n 1. Container 故障：Resource Manager 可以分配其他的 Container 继续执行 \n 2. App Master故障：分配新的 Container，启动 App Master，新的 App Master 从 App Manager 获取相关恢复信\n 3. NodeManager 故障：移除这个节点，在其他的 NodeManager 重启继续任务。 \n 4. ResourceManager 故障：在Yarn 集群中，ResourceManager 可以启动多台，只有其中一台是 active 状态的，其他都处于待命状态。 这台active 状态的 ResourceManager 执行的时候会向 ZooKeeper 集群写入它的状态； 当它故障的时候这些 RM首先选举出另外一台 leader 变为 active 状态，然后从 ZooKeeper 集群加载 ResourceManager的状态； 在转移的过程中它不接收新的 Job，转移完成后才接收新 Job。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200622115325805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\nResourceManager HA 由一对 Active，Standby节点构成，通过RMStataStore存储内部数据和主要应用的数据及标记。\n目前支持的可替代的 RMStateStore实现有：基于内存的 MemoryRMStateStore，基于文件系统的FileSystemRMStateStore，及基于Zookeeper的ZKRMStateStore。\nResourceManager HA的架构模式同NameNode HA的架构模式基本一致，数据共享由RMStateStore，而ZKFC称为ResourceManager进程的一个服务，非独立存在。\n\n\n参考文章：\n - [https://blog.csdn.net/u013063153/article/details/72956900](https://blog.csdn.net/u013063153/article/details/72956900)\n - [https://blog.csdn.net/m0_37666500/article/details/87832272](https://blog.csdn.net/m0_37666500/article/details/87832272)\n - [https://blog.csdn.net/qq_38265137/article/details/80382378](https://blog.csdn.net/qq_38265137/article/details/80382378)\n - [https://www.cnblogs.com/lillcol/p/11157556.html](https://www.cnblogs.com/lillcol/p/11157556.html)\n - [https://www.jianshu.com/p/3f406cf438be](https://www.jianshu.com/p/3f406cf438be)\n\n---\n以上内容仅供参考学习，如有侵权请联系我删除！\n如果这篇文章对您有帮助，左下角的大拇指就是对博主最大的鼓励。\n您的鼓励就是博主最大的动力！","tags":["Hadoop"],"categories":["Hadoop学习指南"]},{"title":"MapReduce基本原理（详解！）","url":"/posts/1246879312/","content":"## 一、概念理解\n<!-- more -->\nMapReduce是面向大数据并行处理的计算模型、框架和平台，它隐含了以下三层含义：\n1）MapReduce是一个基于集群的高性能并行计算平台。它允许用市场上普通的商用服务器构成一个包含数十、数百至数千个节点的分布和并行计算集群。\n2）MapReduce是一个并行计算与运行软件框架。它提供了一个庞大但设计精良的并行计算软件框架，能自动完成计算任务的并行化处理，自动划分计算数据和计算任务，在集群节点上自动分配和执行任务以及收集计算结果，将数据分布存储、数据通信、容错处理等并行计算涉及到的很多系统底层的复杂细节交由系统负责处理，大大减少了软件开发人员的负担。\n3）MapReduce是一个并行程序设计模型与方法。它借助于函数式程序设计语言Lisp的设计思想，提供了一种简便的并行程序设计方法，用Map和Reduce两个函数编程实现基本的并行计算任务，提供了抽象的操作和并行编程接口，以简单方便地完成大规模数据的编程和计算处理。\n\n**总结：MapReduce是一个基于集群的计算平台，是一个简化分布式编程的计算框架，是一个将分布式计算抽象为Map和Reduce两个阶段的编程模型。**\n\n**MapReduce核心思想：分而治之**\n\n## 二、MapReduce计算模型\n我们知道MapReduce计算模型主要由三个阶段构成：Map、shuffle、Reduce。\n\nMap是映射，负责数据的过滤分法，将原始数据转化为键值对；Reduce是合并，将具有相同key值的value进行处理后再输出新的键值对作为最终结果。为了让Reduce可以并行处理Map的结果，必须对Map的输出进行一定的排序与分割，然后再交给对应的Reduce，而这个将Map输出进行进一步整理并交给Reduce的过程就是Shuffle。整个MR的大致过程如下：\n\nmapreduce运行原理图解\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621224319651.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621224326861.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621224336197.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/202006212243464.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\nMap和Reduce操作需要我们自己定义相应Map类和Reduce类，以完成我们所需要的化简、合并操作，而shuffle则是系统自动帮我们实现的，了解shuffle的具体流程能帮助我们编写出更加高效的Mapreduce程序。\n\nShuffle过程包含在Map和Reduce两端，即Map shuffle和Reduce shuffle\n\n## 三、Map shuffle\n在Map端的shuffle过程是对Map的结果进行分区、排序、分割，然后将属于同一划分（分区）的输出合并在一起并写在磁盘上，最终得到一个分区有序的文件，分区有序的含义是map输出的键值对按分区进行排列，具有相同partition值的键值对存储在一起，每个分区里面的键值对又按key值进行升序排列（默认），其流程大致如下：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621232347959.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n### （1）Partition\n对于map输出的每一个键值对，系统都会给定一个partition，partition值默认是通过计算key的hash值后对Reduce task的数量取模获得。如果一个键值对的partition值为1，意味着这个键值对会交给第一个Reducer处理。\n\n我们知道每一个Reduce的输出都是有序的，但是将所有Reduce的输出合并到一起却并非是全局有序的，如果要做到全局有序，我们该怎么做呢？最简单的方式，只设置一个Reduce task，但是这样完全发挥不出集群的优势，而且能应对的数据量也很受限。最佳的方式是自己定义一个Partitioner，用输入数据的最大值除以系统Reduce task数量的商作为分割边界，也就是说分割数据的边界为此商的1倍、2倍至numPartitions-1倍，这样就能保证执行partition后的数据是整体有序的。\n\n另一种需要我们自己定义一个Partitioner的情况是各个Reduce task处理的键值对数量极不平衡。对于某些数据集，由于很多不同的key的hash值都一样，导致这些键值对都被分给同一个Reducer处理，而其他的Reducer处理的键值对很少，从而拖延整个任务的进度。当然，编写自己的Partitioner必须要保证具有相同key值的键值对分发到同一个Reducer。\n\n### （2）Collector\nMap的输出结果是由collector处理的，每个Map任务不断地将键值对输出到在内存中构造的一个环形数据结构中。使用环形数据结构是为了更有效地使用内存空间，在内存中放置尽可能多的数据。\n\n这个数据结构其实就是个字节数组，叫Kvbuffer，名如其义，但是这里面不光放置了数据，还放置了一些索引数据，给放置索引数据的区域起了一个Kvmeta的别名，在Kvbuffer的一块区域上穿了一个IntBuffer（字节序采用的是平台自身的字节序）的马甲。数据区域和索引数据区域在Kvbuffer中是相邻不重叠的两个区域，用一个分界点来划分两者，分界点不是亘古不变的，而是每次Spill之后都会更新一次。初始的分界点是0，数据的存储方向是向上增长，索引数据的存储方向是向下增长，如图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621232652612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\nKvbuffer的存放指针bufindex是一直闷着头地向上增长，比如bufindex初始值为0，一个Int型的key写完之后，bufindex增长为4，一个Int型的value写完之后，bufindex增长为8。\n\n索引是对在kvbuffer中的键值对的索引，是个四元组，包括：value的起始位置、key的起始位置、partition值、value的长度，占用四个Int长度，Kvmeta的存放指针Kvindex每次都是向下跳四个“格子”，然后再向上一个格子一个格子地填充四元组的数据。比如Kvindex初始位置是-4，当第一个键值对写完之后，(Kvindex+0)的位置存放value的起始位置、(Kvindex+1)的位置存放key的起始位置、(Kvindex+2)的位置存放partition的值、(Kvindex+3)的位置存放value的长度，然后Kvindex跳到-8位置，等第二个键值对和索引写完之后，Kvindex跳到-12位置。\n\nKvbuffer的大小可以通过io.sort.mb设置，默认大小为100M。但不管怎么设置，Kvbuffer的容量都是有限的，键值对和索引不断地增加，加着加着，Kvbuffer总有不够用的那天，那怎么办？把数据从内存刷到磁盘上再接着往内存写数据，把Kvbuffer中的数据刷到磁盘上的过程就叫Spill，多么明了的叫法，内存中的数据满了就自动地spill到具有更大空间的磁盘。\n\n关于Spill触发的条件，也就是Kvbuffer用到什么程度开始Spill，还是要讲究一下的。如果把Kvbuffer用得死死得，一点缝都不剩的时候再开始Spill，那Map任务就需要等Spill完成腾出空间之后才能继续写数据；如果Kvbuffer只是满到一定程度，比如80%的时候就开始Spill，那在Spill的同时，Map任务还能继续写数据，如果Spill够快，Map可能都不需要为空闲空间而发愁。两利相衡取其大，一般选择后者。Spill的门限可以通过io.sort.spill.percent，默认是0.8。\n\nSpill这个重要的过程是由Spill线程承担，Spill线程从Map任务接到“命令”之后就开始正式干活，干的活叫SortAndSpill，原来不仅仅是Spill，在Spill之前还有个颇具争议性的Sort。\n### （3）Sort\n当Spill触发后，SortAndSpill先把Kvbuffer中的数据按照partition值和key两个关键字升序排序，移动的只是索引数据，排序结果是Kvmeta中数据按照partition为单位聚集在一起，同一partition内的按照key有序。\n### （4）Spill\nSpill线程为这次Spill过程创建一个磁盘文件：从所有的本地目录中轮训查找能存储这么大空间的目录，找到之后在其中创建一个类似于“spill12.out”的文件。Spill线程根据排过序的Kvmeta挨个partition的把数据吐到这个文件中，一个partition对应的数据吐完之后顺序地吐下个partition，直到把所有的partition遍历完。一个partition在文件中对应的数据也叫段(segment)。在这个过程中如果用户配置了combiner类，那么在写之前会先调用combineAndSpill()，对结果进行进一步合并后再写出。Combiner会优化MapReduce的中间结果，所以它在整个模型中会多次使用。那哪些场景才能使用Combiner呢？Combiner的输出是Reducer的输入，Combiner绝不能改变最终的计算结果。所以从我的想法来看，Combiner只应该用于那种Reduce的输入key/value与输出key/value类型完全一致，且不影响最终结果的场景。比如累加，最大值等。Combiner的使用一定得慎重，如果用好，它对job执行效率有帮助，反之会影响reduce的最终结果。\n\n所有的partition对应的数据都放在这个文件里，虽然是顺序存放的，但是怎么直接知道某个partition在这个文件中存放的起始位置呢？强大的索引又出场了。有一个三元组记录某个partition对应的数据在这个文件中的索引：起始位置、原始数据长度、压缩之后的数据长度，一个partition对应一个三元组。然后把这些索引信息存放在内存中，如果内存中放不下了，后续的索引信息就需要写到磁盘文件中了：从所有的本地目录中轮训查找能存储这么大空间的目录，找到之后在其中创建一个类似于“spill12.out.index”的文件，文件中不光存储了索引数据，还存储了crc32的校验数据。spill12.out.index不一定在磁盘上创建，如果内存（默认1M空间）中能放得下就放在内存中，即使在磁盘上创建了，和spill12.out文件也不一定在同一个目录下。每一次Spill过程就会最少生成一个out文件，有时还会生成index文件，Spill的次数也烙印在文件名中。索引文件和数据文件的对应关系如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621232858112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n在Spill线程如火如荼的进行SortAndSpill工作的同时，Map任务不会因此而停歇，而是一无既往地进行着数据输出。Map还是把数据写到kvbuffer中，那问题就来了：只顾着闷头按照bufindex指针向上增长，kvmeta只顾着按照Kvindex向下增长，是保持指针起始位置不变继续跑呢，还是另谋它路？如果保持指针起始位置不变，很快bufindex和Kvindex就碰头了，碰头之后再重新开始或者移动内存都比较麻烦，不可取。Map取kvbuffer中剩余空间的中间位置，用这个位置设置为新的分界点，bufindex指针移动到这个分界点，Kvindex移动到这个分界点的-16位置，然后两者就可以和谐地按照自己既定的轨迹放置数据了，当Spill完成，空间腾出之后，不需要做任何改动继续前进。分界点的转换如下图所示：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621232917639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\nMap任务总要把输出的数据写到磁盘上，即使输出数据量很小在内存中全部能装得下，在最后也会把数据刷到磁盘上。\n\n### （5）Merge\nMap任务如果输出数据量很大，可能会进行好几次Spill，out文件和Index文件会产生很多，分布在不同的磁盘上。最后把这些文件进行合并的merge过程闪亮登场。\n\nMerge过程怎么知道产生的Spill文件都在哪了呢？从所有的本地目录上扫描得到产生的Spill文件，然后把路径存储在一个数组里。Merge过程又怎么知道Spill的索引信息呢？没错，也是从所有的本地目录上扫描得到Index文件，然后把索引信息存储在一个列表里。到这里，又遇到了一个值得纳闷的地方。在之前Spill过程中的时候为什么不直接把这些信息存储在内存中呢，何必又多了这步扫描的操作？特别是Spill的索引数据，之前当内存超限之后就把数据写到磁盘，现在又要从磁盘把这些数据读出来，还是需要装到更多的内存中。之所以多此一举，是因为这时kvbuffer这个内存大户已经不再使用可以回收，有内存空间来装这些数据了。（对于内存空间较大的土豪来说，用内存来省却这两个io步骤还是值得考虑的。）\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621233044508.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n然后为merge过程创建一个叫file.out的文件和一个叫file.out.Index的文件用来存储最终的输出和索引，一个partition一个partition的进行合并输出。对于某个partition来说，从索引列表中查询这个partition对应的所有索引信息，每个对应一个段插入到段列表中。也就是这个partition对应一个段列表，记录所有的Spill文件中对应的这个partition那段数据的文件名、起始位置、长度等等。\n\n然后对这个partition对应的所有的segment进行合并，目标是合并成一个segment。当这个partition对应很多个segment时，会分批地进行合并：先从segment列表中把第一批取出来，以key为关键字放置成最小堆，然后从最小堆中每次取出最小的输出到一个临时文件中，这样就把这一批段合并成一个临时的段，把它加回到segment列表中；再从segment列表中把第二批取出来合并输出到一个临时segment，把其加入到列表中；这样往复执行，直到剩下的段是一批，输出到最终的文件中。最终的索引数据仍然输出到Index文件中。\n## 四、Reduce shuffle\n在Reduce端，shuffle主要分为复制Map输出、排序合并两个阶段。\n### （1）Copy\nReduce任务通过HTTP向各个Map任务拖取它所需要的数据。Map任务成功完成后，会通知父TaskTracker状态已经更新，TaskTracker进而通知JobTracker（这些通知在心跳机制中进行）。所以，对于指定作业来说，JobTracker能记录Map输出和TaskTracker的映射关系。Reduce会定期向JobTracker获取Map的输出位置，一旦拿到输出位置，Reduce任务就会从此输出对应的TaskTracker上复制输出到本地，而不会等到所有的Map任务结束。\n\n### （2）Merge Sort\nCopy过来的数据会先放入内存缓冲区中，如果内存缓冲区中能放得下这次数据的话就直接把数据写到内存中，即内存到内存merge。Reduce要向每个Map去拖取数据，在内存中每个Map对应一块数据，当内存缓存区中存储的Map数据占用空间达到一定程度的时候，开始启动内存中merge，把内存中的数据merge输出到磁盘上一个文件中，即内存到磁盘merge。在将buffer中多个map输出合并写入磁盘之前，如果设置了Combiner，则会化简压缩合并的map输出。Reduce的内存缓冲区可通过mapred.job.shuffle.input.buffer.percent配置，默认是JVM的heap size的70%。内存到磁盘merge的启动门限可以通过mapred.job.shuffle.merge.percent配置，默认是66%。\n\n当属于该reducer的map输出全部拷贝完成，则会在reducer上生成多个文件（如果拖取的所有map数据总量都没有内存缓冲区，则数据就只存在于内存中），这时开始执行合并操作，即磁盘到磁盘merge，Map的输出数据已经是有序的，Merge进行一次合并排序，所谓Reduce端的sort过程就是这个合并的过程。一般Reduce是一边copy一边sort，即copy和sort两个阶段是重叠而不是完全分开的。最终Reduce shuffle过程会输出一个整体有序的数据块。\n\n\n参考文章：\n\n - [https://blog.csdn.net/u014374284/article/details/49205885](https://blog.csdn.net/u014374284/article/details/49205885)（推荐阅读）\n - [https://www.jianshu.com/p/ca165beb305b](https://www.jianshu.com/p/ca165beb305b)\n\n---\n以上内容仅供参考学习，如有侵权请联系我删除！\n如果这篇文章对您有帮助，左下角的大拇指就是对博主最大的鼓励。\n您的鼓励就是博主最大的动力！\n\n","tags":["Hadoop"],"categories":["Hadoop学习指南"]},{"title":"Kafka基本原理详解（超详细！）","url":"/posts/1153574719/","content":"## 一、概念理解 \n<!-- more -->\n**Kafka**是最初由Linkedin公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于zookeeper协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于hadoop的批处理系统、低延迟的实时系统、storm/Spark流式处理引擎，web/nginx日志、访问日志，消息服务等等，用scala语言编写，Linkedin于2010年贡献给了Apache基金会并成为顶级开源 项目。\n\n### （1）产生背景\n当今社会各种应用系统诸如商业、社交、搜索、浏览等像信息工厂一样不断的生产出各种信息，在大数据时代，我们面临如下几个挑战：\n\n 1. 如何收集这些巨大的信息 \n 2. 如何分析它 \n 3. 如何及时做到如上两点\n\n以上几个挑战形成了一个业务需求模型，即生产者生产（produce）各种信息，消费者消费（consume）（处理分析）这些信息，而在生产者与消费者之间，需要一个沟通两者的桥梁-消息系统。从一个微观层面来说，这种需求也可理解为不同的系统之间如何传递消息。\n\n**Kafka诞生**\nKafka由 linked-in 开源\nkafka-即是解决上述这类问题的一个框架，它实现了生产者和消费者之间的无缝连接。\nkafka-高产出的分布式消息系统(A high-throughput distributed messaging system)\n\n### （2）Kafka的特性\n\n - **高吞吐量、低延迟**：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒 \n - **可扩展性**：kafka集群支持热扩展\n - **持久性、可靠性**：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失 \n - **容错性**：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）\n - **高并发**：支持数千个客户端同时读写\n\n### （3）Kafka场景应用\n\n - **日志收集**：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、Hbase、Solr等。\n - **消息系统**：解耦和生产者和消费者、缓存消息等。\n - **用户活动跟踪**：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。\n - **运营指标**：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。\n - **流式处理**：比如spark streaming和storm \n - **事件源**\n\n### （4）Kafka一些重要设计思想\n\n - **Consumergroup**：各个consumer可以组成一个组，每个消息只能被组中的一个consumer消费，如果一个消息可以被多个consumer消费的话，那么这些consumer必须在不同的组。\n - **消息状态**：在Kafka中，消息的状态被保存在consumer中，broker不会关心哪个消息被消费了被谁消费了，只记录一个offset值（指向partition中下一个要被消费的消息位置），这就意味着如果consumer处理不好的话，broker上的一个消息可能会被消费多次。\n - **消息持久化**：Kafka中会把消息持久化到本地文件系统中，并且保持极高的效率。\n - **消息有效期**：Kafka会长久保留其中的消息，以便consumer可以多次消费，当然其中很多细节是可配置的。\n - **批量发送**：Kafka支持以消息集合为单位进行批量发送，以提高push效率。 \n - **push-and-pull** :Kafka中的Producer和consumer采用的是push-and-pull模式，即Producer只管向broker push消息，consumer只管从broker pull消息，两者对消息的生产和消费是异步的。\n - **Kafka集群中broker之间的关系**：不是主从关系，各个broker在集群中地位一样，我们可以随意的增加或删除任何一个broker节点。\n - **负载均衡方面**： Kafka提供了一个 metadata API来管理broker之间的负载（对Kafka0.8.x而言，对于0.7.x主要靠zookeeper来实现负载均衡）。\n - **同步异步**：Producer采用异步push方式，极大提高Kafka系统的吞吐率（可以通过参数控制是采用同步还是异步方式）。\n - **分区机制partition**：Kafka的broker端支持消息分区，Producer可以决定把消息发到哪个分区，在一个分区中消息的顺序就是Producer发送消息的顺序，一个主题中可以有多个分区，具体分区的数量是可配置的。分区的意义很重大，后面的内容会逐渐体现。\n - **离线数据装载**：Kafka由于对可拓展的数据持久化的支持，它也非常适合向Hadoop或者数据仓库中进行数据装载。\n - **插件支持**：现在不少活跃的社区已经开发出不少插件来拓展Kafka的功能，如用来配合Storm、Hadoop、flume相关的插件。\n\n\n## 二、消息队列通信的模式\n### （1）点对点模式\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200624144932564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n如上图所示，点对点模式通常是基于拉取或者轮询的消息传送模型，这个模型的特点是发送到队列的消息被一个且只有一个消费者进行处理。生产者将消息放入消息队列后，由消费者主动的去拉取消息进行消费。点对点模型的的优点是消费者拉取消息的频率可以由自己控制。但是消息队列是否有消息需要消费，在消费者端无法感知，所以在消费者端需要额外的线程去监控。\n### （2）发布订阅模式\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020062414495136.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n如上图所示，发布订阅模式是一个基于消息送的消息传送模型，改模型可以有多种不同的订阅者。生产者将消息放入消息队列后，队列会将消息推送给订阅过该类消息的消费者（类似微信公众号）。由于是消费者被动接收推送，所以无需感知消息队列是否有待消费的消息！但是consumer1、consumer2、consumer3由于机器性能不一样，所以处理消息的能力也会不一样，但消息队列却无法感知消费者消费的速度！所以推送的速度成了发布订阅模模式的一个问题！假设三个消费者处理速度分别是8M/s、5M/s、2M/s，如果队列推送的速度为5M/s，则consumer3无法承受！如果队列推送的速度为2M/s，则consumer1、consumer2会出现资源的极大浪费！\n\n## 三、Kafka的架构原理\n上面简单的介绍了为什么需要消息队列以及消息队列通信的两种模式，下面主角介绍Kafka。Kafka是一种**高吞吐量的分布式发布订阅消息系统**，它可以处理消费者规模的网站中的所有动作流数据，具有高性能、持久化、多副本备份、横向扩展能力。。\n\n### （1）基础架构与名词解释\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200624145628396.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n\n - Producer：Producer即生产者，消息的产生者，是消息的入口。\n - Broker：Broker是kafka实例，每个服务器上有一个或多个kafka的实例，我们姑且认为每个broker对应一台服务器。每个kafka集群内的broker都有一个不重复的编号，如图中的broker-0、broker-1等……\n - Topic：消息的主题，可以理解为消息的分类，kafka的数据就保存在topic。在每个broker上都可以创建多个topic。\n - Partition：Topic的分区，每个topic可以有多个分区，分区的作用是做负载，提高kafka的吞吐量。同一个topic在不同的分区的数据是不重复的，partition的表现形式就是一个一个的文件夹！\n - Replication:每一个分区都有多个副本，副本的作用是做备胎。当主分区（Leader）故障的时候会选择一个备胎（Follower）上位，成为Leader。在kafka中默认副本的最大数量是10个，且副本的数量不能大于Broker的数量，follower和leader绝对是在不同的机器，同一机器对同一个分区也只可能存放一个副本（包括自己）。\n - Message：每一条发送的消息主体。\n - Consumer：消费者，即消息的消费方，是消息的出口。\n - Consumer Group：我们可以将多个消费组组成一个消费者组，在kafka的设计中同一个分区的数据只能被消费者组中的某一个消费者消费。同一个消费者组的消费者可以消费同一个topic的不同分区的数据，这也是为了提高kafka的吞吐量！\n - Zookeeper：kafka集群依赖zookeeper来保存集群的的元信息，来保证系统的可用性。\n\n### （2）工作流程分析\n#### （1）发送数据\n我们看上面的架构图中，producer就是生产者，是数据的入口。注意看图中的红色箭头，**Producer在写入数据的时候永远的找leader**，不会直接将数据写入**follower**！那leader怎么找呢？写入的流程又是什么样的呢？我们看下图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200624150617430.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n发送的流程就在图中已经说明了，就不单独在文字列出来了！需要注意的一点是，消息写入leader后，follower是主动的去leader进行同步的！producer采用push模式将数据发布到broker，每条消息追加到分区中，顺序写入磁盘，所以保证同一分区内的数据是有序的！写入示意图如下：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200624150636117.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n上面说到数据会写入到不同的分区，那kafka为什么要做分区呢？相信大家应该也能猜到，分区的主要目的是：\n\n 1. **方便扩展**：因为一个topic可以有多个partition，所以我们可以通过扩展机器去轻松的应对日益增长的数据量。\n 2. **提高并发**：以partition为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率。\n\n熟悉负载均衡的朋友应该知道，当我们向某个服务器发送请求的时候，服务端可能会对请求做一个负载，将流量分发到不同的服务器，那在kafka中，如果某个topic有多个partition，producer又怎么知道该将数据发往哪个partition呢？kafka中有几个原则：\n\n 1. partition在写入的时候可以指定需要写入的partition，如果有指定，则写入对应的partition。 　\n 2. 如果没有指定partition，但是设置了数据的key，则会根据key的值hash出一个partition。 　　\n 3. 如果既没指定partition，又没有设置key，则会轮询选出一个partition。\n\n保证消息不丢失是一个消息队列中间件的基本保证，那producer在向kafka写入消息的时候，怎么保证消息不丢失呢？其实上面的写入流程图中有描述出来，那就是通过ACK应答机制！在生产者向队列写入数据的时候可以设置参数来确定是否确认kafka接收到数据，这个参数可设置的值为**0、1、all**。\n\n - 0代表producer往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是效率最高。\n - 1代表producer往集群发送数据只要leader应答就可以发送下一条，只确保leader发送成功。\n - all代表producer往集群发送数据需要所有的follower都完成从leader的同步才会发送下一条，确保leader发送成功和所有的副本都完成备份。安全性最高，但是效率最低。\n\n最后要注意的是，如果往不存在的topic写数据，能不能写入成功呢？kafka会自动创建topic，分区和副本的数量根据默认配置都是1。\n#### （2）保存数据\nProducer将数据写入kafka后，集群就需要对数据进行保存了！kafka将数据保存在磁盘，可能在我们的一般的认知里，写入磁盘是比较耗时的操作，不适合这种高并发的组件。Kafka初始会单独开辟一块磁盘空间，顺序写入数据（效率比随机写入高）。\n##### （1）Partition 结构\n前面说过了每个topic都可以分为一个或多个partition，如果你觉得topic比较抽象，那partition就是比较具体的东西了！Partition在服务器上的表现形式就是一个一个的文件夹，每个partition的文件夹下面会有多组segment文件，每组segment文件又包含.index文件、.log文件、.timeindex文件（早期版本中没有）三个文件， log文件就实际是存储message的地方，而index和timeindex文件为索引文件，用于检索消息。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200624170905606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n如上图，这个partition有三组segment文件，每个log文件的大小是一样的，但是存储的message数量是不一定相等的（每条的message大小不一致）。文件的命名是以该segment最小offset来命名的，如000.index存储offset为0~368795的消息，kafka就是利用分段+索引的方式来解决查找效率的问题。\n\n##### （2）Message结构\n上面说到log文件就实际是存储message的地方，我们在producer往kafka写入的也是一条一条的message，那存储在log中的message是什么样子的呢？消息主要包含消息体、消息大小、offset、压缩类型……等等！我们重点需要知道的是下面三个：\n\n - **offset**：offset是一个占8byte的有序id号，它可以唯一确定每条消息在parition内的位置！\n - **消息大小**：消息大小占用4byte，用于描述消息的大小。\n - **消息体**：消息体存放的是实际的消息数据（被压缩过），占用的空间根据具体的消息而不一样。\n\n\n##### （3）存储策略\n无论消息是否被消费，kafka都会保存所有的消息。那对于旧数据有什么删除策略呢？\n\n - 基于时间，默认配置是168小时（7天）。\n - 基于大小，默认配置是1073741824。\n\n需要注意的是，kafka读取特定消息的时间复杂度是O(1)，所以这里删除过期的文件并不会提高kafka的性能！\n#### （3）消费数据\n消息存储在log文件后，消费者就可以进行消费了。在讲消息队列通信的两种模式的时候讲到过点对点模式和发布订阅模式。Kafka采用的是点对点的模式，消费者主动的去kafka集群拉取消息，与producer相同的是，消费者在拉取消息的时候也是找**leader**去拉取。\n\n多个消费者可以组成一个消费者组（consumer group），每个消费者组都有一个组id！同一个消费组者的消费者可以消费同一topic下不同分区的数据，但是不会组内多个消费者消费同一分区的数据！！！我们看下图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200624171149382.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n图示是消费者组内的消费者小于partition数量的情况，所以会出现某个消费者消费多个partition数据的情况，消费的速度也就不及只处理一个partition的消费者的处理速度！如果是消费者组的消费者多于partition的数量，那会不会出现多个消费者消费同一个partition的数据呢？上面已经提到过不会出现这种情况！多出来的消费者不消费任何partition的数据。所以在实际的应用中，**建议消费者组的consumer的数量与partition的数量一致**！\n\n在保存数据的小节里面，我们聊到了partition划分为多组segment，每个segment又包含.log、.index、.timeindex文件，存放的每条message包含offset、消息大小、消息体……我们多次提到segment和offset，查找消息的时候是怎么利用segment+offset配合查找的呢？假如现在需要查找一个offset为368801的message是什么样的过程呢？我们先看看下面的图：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200624171235860.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n\n 1. 先找到offset的368801message所在的segment文件（利用二分法查找），这里找到的就是在第二个segment文件。\n 2. 打开找到的segment中的.index文件（也就是368796.index文件，该文件起始偏移量为368796+1，我们要查找的offset为368801的message在该index内的偏移量为368796+5=368801，所以这里要查找的相对offset为5）。由于该文件采用的是稀疏索引的方式存储着相对offset及对应message物理偏移量的关系，所以直接找相对offset为5的索引找不到，这里同样利用二分法查找相对offset小于或者等于指定的相对offset的索引条目中最大的那个相对offset，所以找到的是相对offset为4的这个索引。\n 3. 根据找到的相对offset为4的索引确定message存储的物理偏移位置为256。打开数据文件，从位置为256的那个地方开始顺序扫描直到找到offset为368801的那条Message。\n\n这套机制是建立在offset为有序的基础上，利用segment+有序offset+稀疏索引+二分查找+顺序查找等多种手段来高效的查找数据！至此，消费者就能拿到需要处理的数据进行处理了。那每个消费者又是怎么记录自己消费的位置呢？在早期的版本中，消费者将消费到的offset维护zookeeper中，consumer每间隔一段时间上报一次，这里容易导致重复消费，且性能不好！在新的版本中消费者消费到的offset已经直接维护在kafk集群的__consumer_offsets这个topic中！\n\n参考文章：\n\n - [https://www.cnblogs.com/sujing/p/10960832.html](https://www.cnblogs.com/sujing/p/10960832.html)（推荐阅读）\n - [https://www.cnblogs.com/sa-dan/p/8241372.html](https://www.cnblogs.com/sa-dan/p/8241372.html)\n\n---\n以上内容仅供参考学习，如有侵权请联系我删除！\n如果这篇文章对您有帮助，左下角的大拇指就是对博主最大的鼓励。\n您的鼓励就是博主最大的动力！\n","tags":["Kafka"],"categories":["Kafka学习指南"]},{"title":"HDFS分布式文件系统架构原理详解","url":"/posts/3480810444/","content":"HDFS(Hadoop Distributed File System)是Hadoop核心组成之一\n<!-- more -->，是分布式计算中数据存储管理的基础，被设计成适合运行在通用硬件上的分布式文件系统。HDFS架构中有两类节点，一类是NameNode，又叫“元数据节点”，另一类是DataNode，又叫“数据节点”，分别执行Master和Worker的具体任务。HDFS是一个(Master/Slave)体系结构，“一次写入，多次读取”。HDFS的设计思想：分而治之—将大文件、大批量文件分布式存放在大量独立的机器上。\n## 一、HDFS的优缺点\n### （1）优点\n - 高容错性。数据保存多个副本，通过增加副本的形式提高容错性，某个副本丢失后，它可以通过其它副本自动恢复。\n - 适合大批量数据处理。处理达到GB、TB，甚至PB级别的数据，处理百万规模以上的文件数量，处理10K节点的规模。\n - 流式文件访问。一次写入多次读取，文件一旦写入不能修改，只能追加，保证数据一致性。\n - 可构建在廉价机器上。通过多副本机制提高可靠性，提供容错和恢复机制。\n### （2）缺点\n不适用HDFS的场景：\n - 低延时数据访问。做不到毫秒级存储数据，但是适合高吞吐率(某一时间内写入大量的数据)的场景。\n - 小文件存储。存储大量小文件会占用NameNode大量的内存来存储文件、目录和块信息。\n - 并发写入、随机读写。一个文件不允许多个线程同时写，仅支持数据追加，不支持文件的随机修改。\n\n## 二、HDFS架构原理\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621210839734.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n**HDFS架构**\n - NameNode\n - DataNode\n - Sencondary NameNode\n\n**数据存储细节**\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621211334122.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n### （1）NameNode详解\nNameNode：就是 master，它是一个主管、管理者。\n\n - 管理 HDFS 的名称空间 \n - 管理数据块（Block）映射信息 \n - 配置副本策略 \n - 处理客户端读写请求。\n\nNamenode 的目录结构：  ${ dfs.name.dir}/current /VERSION\n - /edits （操作日志文件）\n - /fsimage （元数据镜像文件）\n - /fstime （保存最近一次恢复的时间）\n\nNamenode 上保存着 HDFS 的名字空间。对于任何对文件系统元数据产生修改的操作， Namenode 都会使用一种称为 EditLog 的事务日志记录下来。例如，在 HDFS 中创建一个文件， Namenode 就会在 Editlog 中插入一条记录来表示；同样地，修改文件的副本系数也将往 Editlog 插入一条记录。 Namenode 在本地操作系统的文件系统中存储这个 Editlog 。整个文件系统的名 字空间，包括数据块到文件的映射、文件的属性等，都存储在一个称为 FsImage 的文件中，这 个文件也是放在 Namenode 所在的本地文件系统上。\nNamenode 在内存中保存着整个文件系统的名字空间和文件数据块映射(Blockmap) 的映像 。这个关键的元数据结构设计得很紧凑，因而一个有 4G 内存的Namenode 足够支撑大量的文件 和目录。当 Namenode 启动时，它从硬盘中读取Editlog 和 FsImage ，将所有 Editlog 中的事务作 用在内存中的 FsImage 上，并将这个新版本的 FsImage 从内存中保存到本地磁盘上，然后删除 旧的 Editlog ，因为这个旧的 Editlog 的事务都已经作用在 FsImage 上了。这个过程称为一个检查 点(checkpoint) 。在当前实现中，检查点只发生在 Namenode 启动时，在不久的将来将实现支持 周期性的检查点。\n\n### （2）Secondary NameNode详解\nSecondary NameNode：并非 NameNode 的热备。当NameNode 挂掉的时候，它并不能马上替换 NameNode 并提供服务。\n\n - 辅助 NameNode，分担其工作量。 \n - 定期合并 fsimage和fsedits，并推送给NameNode。 \n - 在紧急情况下，可辅助恢复NameNode。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621212822292.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n1. SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件，暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成，上层写日志的函数完全感觉不到差别；\n2. SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文件，并下载到本地的相应目录下；\n3. SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件中的各项更新操作，使得内存中的FsImage保持最新；这个过程就是EditLog和FsImage文件合并；\n4. SecondaryNameNode执行完（3）操作之后，会通过post方式将新的FsImage文件发送到NameNode节点上\n5. NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件，同时将edit.new替换EditLog文件，通过这个过程EditLog就变小了\n\n### （3）HDFS NameSpace详解\nHDFS 支持传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数 现有的文件系统类似：用户可以创建、删除、移动或重命名文件。当前， HDFS 不支持用户磁盘配额和访问权限控制，也不支持硬链接和软链接。但 是 HDFS 架构并不妨碍实现这些特性。\nNamenode 负责维护文件系统命名空间，任何对文件系统名字空间或属 性的修改都将被 Namenode 记录下来。应用程序可以设置 HDFS 保存的文件 的副本数目。文件副本的数目称为文件的副本系数，这个信息也是由 Namenode 保存的。\n### （4）DataNode详解\nDataNode：就是Slave。NameNode 下达命令，DataNode 执行实际的操作。\n\n - 存储实际的数据块。 \n - 执行数据块的读/写操作。\n\nDatanode 将 HDFS 数据以文件的形式存储在本地的文件系统中，它并不知道有 关 HDFS 文件的信息。它把每个 HDFS 数据块存储在本地文件系统的一个单独的文件 中。 Datanode 并不在同一个目录创建所有的文件，实际上，它用试探的方法来确定 每个目录的最佳文件数目，并且在适当的时候创建子目录。在同一个目录中创建所 有的本地文件并不是最优的选择，这是因为本地文件系统可能无法高效地在单个目 录中支持大量的文件。\n当一个 Datanode 启动时，它会扫描本地文件系统，产生一个这些本地文件对应 的所有 HDFS 数据块的列表，然后作为报告发送到 Namenode ，这个报告就是块状态 报告。\n\n### （5）Client详解\nClient：就是客户端。\n\n - 文件切分。文件上传 HDFS 的时候，Client 将文件切分成 一个一个的Block，然后进行存储。 \n - 与 NameNode交互，获取文件的位置信息。 \n - 与 DataNode 交互，读取或者写入数据。 \n - Client 提供一些命令来管理HDFS，比如启动或者关闭HDFS。 \n - Client 可以通过一些命令来访问 HDFS。\n\n### （6）HDFS通信协议\n所有的 HDFS 通讯协议都是构建在 TCP/IP 协议上。客户端通过一个可 配置的端口连接到 Namenode ， 通过 ClientProtocol 与 Namenode 交互。而Datanode 是使用 DatanodeProtocol 与 Namenode 交互。再设计上，DataNode 通过周期性的向 NameNode 发送心跳和数据块来保持和 NameNode 的通信，数据块报告的信息包括数据块的属性，即数据块属于哪 个文件，数据块 ID ，修改时间等， NameNode 的 DataNode 和数据块的映射 关系就是通过系统启动时DataNode 的数据块报告建立的。从 ClientProtocol 和 Datanodeprotocol 抽象出一个远程调用 ( RPC ）， 在设计上， Namenode 不会主动发起 RPC ， 而是是响应来自客户端和 Datanode 的 RPC 请求。\n### （7）HDFS的安全模式\nNamenode 启动后会进入一个称为安全模式的特殊状态。处于安全模式 的Namenode 是不会进行数据块的复制的。 Namenode 从所有的 Datanode 接收心跳信号和块状态报告。块状态报告包括了某个 Datanode 所有的数据 块列表。每个数据块都有一个指定的最小副本数。当 Namenode 检测确认某 个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全 (safely replicated) 的；在一定百分比（这个参数可配置）的数据块被 Namenode 检测确认是安全之后（加上一个额外的 30 秒等待时间）， Namenode 将退出安全模式状态。接下来它会确定还有哪些数据块的副本没 有达到指定数目，并将这些数据块复制到其他 Datanode上。\n\n## 三、HDFS文件读写的解析\n### （1）文件读取流程\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020062121514733.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\nHDFS的文件读取原理，主要包括以下几个步骤：\n\n 1. 首先调用FileSystem对象的open方法，其实获取的是一个DistributedFileSystem的实例。\n 2. DistributedFileSystem通过RPC(远程过程调用)获得文件的第一批block的locations，同一block按照重复数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面。\n 3. 前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream就会找出离客户端最近的datanode并连接datanode。\n 4. 数据从datanode源源不断的流向客户端。\n 5. 如果第一个block块的数据读完了，就会关闭指向第一个block块的datanode连接，接着读取下一个block块。这些操作对客户端来说是透明的，从客户端的角度来看只是读一个持续不断的流。\n 6. 如果第一批block都读完了，DFSInputStream就会去namenode拿下一批blocks的location，然后继续读，如果所有的block块都读完，这时就会关闭掉所有的流。\n\n### （2）文件写入流程\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621220758538.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\nHDFS的文件写入原理，主要包括以下几个步骤：\n\n 1. 客户端通过调用 DistributedFileSystem 的create方法，创建一个新的文件。\n 2. DistributedFileSystem 通过 RPC（远程过程调用）调用NameNode，去创建一个没有blocks关联的新文件。创建前，NameNode会做各种校验，比如文件是否存在，客户端有无权限去创建等。如果校验通过，NameNode 就会记录下新文件，否则就会抛出IO异常。\n 3. 前两步结束后会返回 FSDataOutputStream 的对象，和读文件的时候相似，FSDataOutputStream 被封装DFSOutputStream，DFSOutputStream 可以协调 NameNode和DataNode。客户端开始写数据到DFSOutputStream,DFSOutputStream会把数据切成一个个小packet，然后排成队列data queue。 \n 4. DataStreamer 会去处理接受 data queue，它先问询 NameNode 这个新的 block最适合存储的在哪几个DataNode里，比如重复数是3，那么就找到3个最适合的 DataNode，把它们排成一个pipeline。DataStreamer 把 packet 按队列输出到管道的第一个 DataNode 中，第一个DataNode又把 packet 输出到第二个 DataNode 中，以此类推。 \n 5. DFSOutputStream 还有一个队列叫ack queue，也是由 packet组成，等待DataNode的收到响应，当pipeline中的所有DataNode都表示已经收到的时候，这时akcqueue才会把对应的packet包移除掉。 客户端完成写数据后，调用close方法关闭写入流。 \n 6. DataStreamer把剩余的包都刷到 pipeline 里，然后等待 ack 信息，收到最后一个 ack 后，通知 DataNode 把文件标示为已完成。\n\n**流水线复制：**\n当客户端向 HDFS 文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副 本系数设置为 3 ，当本地临时文件累积到一个数据块的大小时，客户端会从 Namenode 获取一个 Datanode 列表用于存放副本。然后客户端开始向第一个 Datanode 传输数据，第一个 Datanode 一小部分一小部分 (4 KB) 地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中 第二个 Datanode节点。第二个 Datanode 也是这样，一小部分一小部分地接收数据，写入本地 仓库，并同时传给第三个 Datanode 。最后，第三个 Datanode 接收数据并存储在本地。因此， Datanode 能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的 方式从前一个 Datanode 复制到下一个\n\n**更细节的原理：**\n客户端创建文件的请求其实并没有立即发送给 Namenode ，事实上，在刚开始阶 段 HDFS 客户端会先将文件数据缓存到本地的一个临时文件。应用程序的写操作被透 明地重定向到这个临时文件。当这个临时文件累积的数据量超过一个数据块的大小 ，客户端才会联系 Namenode 。 Namenode 将文件名插入文件系统的层次结构中，并 且分配一个数据块给它。然后返回 Datanode 的标识符和目标数据块给客户端。接着 客户端将这块数据从本地临时文件上传到指定的 Datanode 上。当文件关闭时，在临 时文件中剩余的没有上传的数据也会传输到指定的 Datanode 上。然后客户端告诉 Namenode 文件已经关闭。此时 Namenode 才将文件创建操作提交到日志里进行存储 。如果 Namenode 在文件关闭前宕机了，则该文件将丢失。\n\n## 四、副本机制\n特点：\n1. 数据类型单一\n2. 副本数比较多\n3. 写文件时副本的放置方法\n4. 动态的副本创建策略\n5. 弱化的副本一致性要求\n\n副本摆放策略：\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621215814953.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n\n参考文章：\n\n - [https://www.cnblogs.com/codeOfLife/p/5375120.html](https://www.cnblogs.com/codeOfLife/p/5375120.html)\n - [https://blog.csdn.net/weixin_38750084/article/details/82963235](https://blog.csdn.net/weixin_38750084/article/details/82963235)\n - [https://www.iteye.com/blog/aoyouzi-2291871](https://www.iteye.com/blog/aoyouzi-2291871)\n\n---\n以上内容仅供参考学习，如有侵权请联系我删除！\n如果这篇文章对您有帮助，左下角的大拇指就是对博主最大的鼓励。\n您的鼓励就是博主最大的动力！\n","tags":["Hadoop"],"categories":["Hadoop学习指南"]},{"title":"HBase常用命令大全","url":"/posts/2685666011/","content":"### （1）进入HBase\n<!-- more -->\n命令：\n```bash\nhbase shell\n```\n### （2）查看服务器状态\n命令：\n```bash\nstatus\n```\n### （3）查询HBase版本\n命令：\n\n```bash\nversion\n```\n### （4）查看有哪些表\n命令：\n\n```bash\nlist\n```\n### （5）创建表\n命令表达式：\n\n```bash\ncreate '表名称', '列族名称1', '列族名称2', '列族名称N'\n```\n### （6）添加一个列族\n命令表达式：\n\n```bash\nalter '表名称', '列族名称'\n```\n### （7）删除列族\n命令表达式：\n\n```bash\nalter '表名称', {NAME => '列族名称', METHOD => 'delete'}\n```\n### （8）启用指定的表\n命令表达式：\n\n```bash\nenable '表名称'\n```\n### （9）禁用指定的表\n命令表达式：\n\n```bash\ndisable '表名称'\n```\n### （10）查看表是否启用\n命令表达式：\n\n```bash\nis_enabled '表名称'\n```\n### （11）查看表是否禁用\n命令表达式：\n\n```bash\nis_disabled '表名称'\n```\n### （12）删除一张表\n第一步屏蔽该表：\n\n```bash\ndisable '表名称'\n```\n\n第二部删除该表：\n\n```bash\ndrop '表名称'\n```\n### （13）查看表的结构\n命令：\n\n```bash\ndescribe '表名称'\n```\n### （14）检查表是否存在\n命令表达式：\n\n```bash\nexists '表名称'\n```\n### （15）删除表中的记录\n命令表达式：\n\n```bash\ndelete '表名称', '行键', '列族名称:列名称'\n```\n### （16）删除整行的值\n命令表达式：\n\n```bash\ndeleteall '表名称', '行键'\n```\n\n### （17）更新表中记录\n命令表达式：\n\n```bash\nput '表名称', '行键', '列族名称:列名称', '值'\n```\n### （18）查看行键中记录\n命令表达式：\n\n```bash\nget '表名称', '行键'\n```\n### （19）查看表中记录数\n命令表达式：\n\n```bash\ncount '表名称'\n```\n### （20）扫描整张表\n命令表达式：\n\n```bash\nscan '表名称'\n```\n### （21）扫描整个列族\n命令表达式：\n\n```bash\nscan '表名称', {COLUMN  => '列族名称'}\n```\n### （22）查看指定表中某个列族的所有数据\n命令表达式：\n\n```bash\nscan '表名称', {COLUMNS  => '列族名称:列名称'}\n```\n### （23）限制查询结果的条数\n命令表达式：\n\n```bash\nscan '表名称', {STARTROW => '开始行', LIMIT => 行数, VERSIONS => 版本号, STOPROW => 结束行, TIMERANGE => '限制时间戳范围'}\n```\n### （24）使用行键RowFilter过滤进行搜索（binary）\n命令表达式：\n```bash\nscan '表名称', FILTER=>\"RowFilter(=, 'binary:rowkey值')\"\n```\n### （25）使用行键RowFilter过滤进行搜索（substring）\n命令表达式：\n\n```bash\nscan '表名称', FILTER=>\"RowFilter(=, 'substring:子串')\"\n```\n### （26）使用等值过滤搜索（binary）\n命令表达式：\n\n```bash\nscan '表名称', FILTER=>\"ValueFilter(=, 'binary:某值')\"\n```\n### （27）使用等值过滤搜索（substring）\n命令表达式：\n\n```bash\nscan '表名称', FILTER=>\"ValueFilter(=, ' substring:某子串')\"\n```\n\n---\n以上内容仅供参考学习，如有侵权请联系我删除！\n如果这篇文章对您有帮助，左下角的大拇指就是对博主最大的鼓励。\n您的鼓励就是博主最大的动力！","tags":["Hadoop"],"categories":["Hadoop学习指南"]},{"title":"HBase的应用场景及架构原理","url":"/posts/2845542846/","content":"## 一、HBase在实际业务场景中的应用\n<!-- more -->\n HBase是一个构建在HDFS上的分布式列存储系统；HBase是Apache Hadoop生态系统中的重要一员，主要用于海量结构化数据存储\nHBase能做什么？\n\n - 海量数据存储\n - 准实时查询\n\n举例说明HBase在实际业务场景中的应用\n\n - 交通\n - 金融\n - 电商\n - 移动\n\n## 二、HBase的特点\n\n - 容量大：HBase单表可以有百亿行，百万列，数据矩阵横向和纵向两个纬度所支持的数据量级别都非常具有弹性\n - 稀疏性：为空的列并不占用存储空间，表可以设计的非常稀疏\n - 多版本：HBase每一列的数据存储有多个Version\n - 面向列：HBase是面向列的存储和权限控制，并支持独立检索。列式存储，其数据在表中是按照某列存储的，这样在查询只需要少数几个字段的时候，能大大减少读取的数据量。\n - 扩展性：底层依赖于HDFS\n - 高可靠性：WAL机制保证了数据写入时不会因集群异常而导致写入数据丢失：Replication机制保证了在集群出现严重的问题时，数据不会发生丢失或损坏。而HBase底层使用HDFS，HDFS本身也有备份。\n - 高性能：底层的LSM数据结构和RowKey有序排列等架构上的独特设计，使得HBase具有非常的写入性能。region切分、主键索引和缓存机制使得HBase在海量数据下具备一定的随机读取性能，该性能针对Rowkey的查询能够达到毫秒级。\n\n## 三、HBase数据模型并举例说明\n### （1）逻辑存储模型\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620144549427.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620144556770.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620145009724.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620145026967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n RowKey：Hbase使用Rowkey来唯一的区分某一行的数据。\n Column Family（列族）：Hbase通过列族划分数据的存储，列族下面可以包含任意多的列，实现灵活的数据存取。Hbase的列族不是越多越好，官方推荐的是列族最好小于或者等于3。我们使用的场景一般是1个列族。\n Time Stamp（时间戳）：TimeStamp对Hbase来说至关重要，因为它是实现Hbase多版本的关键。在Hbase中使用不同的timestame来标识相同rowkey行对应的不通版本的数据。\nCell：HBase 中通过 rowkey 和 columns 确定的为一个存储单元称为 cell。每个 cell 都保存着同一份 数据的多个版本。版本通过时间戳来索引。\n\n### （2）物理存储模型\nHbase的Table中的所有行都按照row key的字典序排列。Table 在行的方向上分割为多个Region。Region按大小分割的，每个表开始只有一个region，随 着数据增多，region不断增大，当增大到一个阀值的时候， region就会等分会两个新的region，之后会有越来越多的 region。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2020062015133549.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620151359707.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)Region是HBase中分布式存储和负载均衡的最小单元。 不同Region分布到不同RegionServer上。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620151423495.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70) Region虽然是分布式存储的最小单元，但并不是存储 的最小单元。Region由一个或者多个Store组成，每个store保存一个 columns family。每个Strore又由一个memStore和0至多个StoreFile组成。memStore存储在内存中，StoreFile存储在HDFS上。 \n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620151508530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n## 四、HBase基本架构\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620151645506.png)\n包括了HMaster、HRegionSever、HRegion、HLog、Store、MemStore、StoreFile、HFile等。HBase底层依赖HDFS，通过DFS Cilent进行HDFS操作。HMaster负责把HRegion分配给HRegionServer，每一个HRegionServer可以包含多个HRegion，多个HRegion共享HLog，HLog用来做灾难恢复。每一个HRegion由一个或多个Store组成，一个Store对应表的一个列族，每个Store中包含与其对应的MemStore以及一个或多个StoreFile（是实际数据存储文件HFile的轻量级封装），MemStore是在内存中的，保存了修改的数据，MemStore中的数据写到文件中就是StoreFile。\n\n### （1）HMaster\n HMaster的主要功能有：\n\n - 把HRegion分配到某一个RegionServer。\n - 有RegionServer宕机了，HMaster可以把这台机器上的Region迁移到active的RegionServer上。\n - 对HRegionServer进行负载均衡。\n - 通过HDFS的dfs client接口回收垃圾文件（无效日志等）\n注：HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行。\n\n### （2）HRegionServer\nHRegionServer的主要功能有：\n - 维护HMaster分配给它的HRegion，处理对这些HRegion的IO请求，也就是说客户端直接和HRegionServer打交道。（从图中也能看出来）\n - 负责切分正在运行过程中变得过大的HRegion\n\n### （3）基本架构\nHBase构建在HDFS之上，其组件包括 Client、zookeeper、HDFS、Hmaster以及HRegionServer。Client包含访问HBase的接口，并维护cache来加快对HBase的访问。Zookeeper用来保证任何时候，集群中只有一个master，存贮所有Region的寻址入口以及实时监控Region server的上线和下线信息。并实时通知给Master存储HBase的schema和table元数据。HMaster负责为Region server分配region和Region server的负载均衡。如果发现失效的Region server并重新分配其上的region。同时，管理用户对table的增删改查操作。Region Server 负责维护region，处理对这些region的IO请求并且切分在运行过程中变得过大的region。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620152844598.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\nHBase 依赖ZooKeeper，默认情况下，HBase 管理ZooKeeper 实例。比如， 启动或者停止ZooKeeper。Master与RegionServers 启动时会向ZooKeeper注册。因此，Zookeeper的引入使得 Master不再是单点故障。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620152948461.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n Client每次写数据库之前，都会首先血Hlog日志。记录写操作。如果不做日志记录，一旦发生故障，操作将不可恢复。HMaster一旦故障，Zookeeper将重新选择一个新的Master 。无Master过程中，数据读取仍照常进行。但是，无master过程中，region切分、负载均衡等无法进行。RegionServer出现故障的处理原理是定时向Zookeeper汇报心跳，如果一旦时 间内未出现心跳HMaster将该RegionServer上的Region重新分配到其他RegionServer上。失效服务器上“预写”日志由主服务器进行分割并派送给新的 RegionServer 。Zookeeper是一个可靠地服务，一般配置3或5个Zookeeper实例。 \n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620152948339.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n  寻找RegionServer定位的顺序是ZooKeeper --ROOT-(单Region) -.META. -用户表 。如上图所示。-ROOT- 表包含.META.表所在的region列表，该表只会有一 个Region。 Zookeeper中记录了-ROOT-表的location。  .META. 表包含所有的用户空间region列表，以及 RegionServer的服务器地址。 \n\n\n参考文章：\n\n - [https://www.jianshu.com/p/9d3d388eae19?utm_source=oschina-app](https://www.jianshu.com/p/9d3d388eae19?utm_source=oschina-app)\n - [https://blog.csdn.net/tianyeshiye/article/details/80768072](https://blog.csdn.net/tianyeshiye/article/details/80768072)","tags":["Hadoop"],"categories":["Hadoop学习指南"]},{"title":"Centos7下HBase安装与配置（亲测！）","url":"/posts/3651396382/","content":"## Centos7下Hadoop完全分布式安装\n<!-- more -->\n - 电脑系统：macOS 10.15.4 \n - 虚拟机软件：Parallels Desktop14 \n - Hadoop各节点节点操作系统：CentOS 7\n - JDK版本：jdk1.8.0_162 \n - HBase版本：hbase-1.2.0-cdh5.9.3\n\nhbase的下载源地址：\n官网：\n[https://archive.cloudera.com/cdh5/cdh/5/](https://archive.cloudera.com/cdh5/cdh/5/)\nCDH版本：\n[https://archive.apache.org/dist/hadoop/](https://archive.apache.org/dist/hadoop/)\n## 第一步：安装软件\n### （1）上传文件\n将本机的安装包上传到虚拟机node1，上传方式：\n\n```bash\nscp 本机的文件绝对路径 caizhengjie@10.211.55.49:/opt/Hadoop\n```\n### （2）解压文件\n上传成功之后需要对文件赋予权限\n\n```bash\nchmod u+x hbase-1.2.0-cdh5.9.3.tar.gz\n```\n解压文件：\n\n```bash\ntar -zxvf hbase-1.2.0-cdh5.9.3.tar.gz\n```\n创建软链接：\n```bash\nln -s hbase-1.2.0-cdh5.9.3 hbase\n```\n\n## 第二步：配置环境变量\n\n```bash\nvim ~/.bashrc\n```\n然后添加以下内容，注意三台虚拟机都需要配置环境变量\n```bash\nexport HBASE_HOME=/opt/Hadoop/hbase\nexport PATH=$HBASE_HOME/bin:$PATH\n```\n最后使之生效\n\n```bash\nsource ~/.bashrc\n```\n\n## 第三步：修改配置文件\n### （1）修改hbase-env.sh配置文件\n**第一步**\n进入到conf目录下\n\n```bash\ncd /opt/Hadoop/hbase/conf\nvim hbase-env.sh\n```\n找到**export JAVA_HOME**，将前面的**#**去掉\n修改为：\n\n```bash\nexport JAVA_HOME=/opt/Hadoop/jdk1.8.0_162\n```\n**第二步**\n接着找到图中红框的部分，将它注释掉\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620120557696.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n原理解释： 启动hbase的时候报出警告\n\n```bash\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0\n```\n查看配置文件\n #Configure PermSize. Only needed in JDK7. You can safely remove it for JDK8+\n 这里我用的是jdk8，按照上面的操作将那两行注释掉就不会报出警告了。\n **第三步**\n 找到**export HBASE_MANAGES_ZK**，将前面的#去掉\n 修改为：\n```bash\nexport HBASE_MANAGES_ZK=false\n```\n原理解释：这里我们不用自带的zookeeper，选择用我们自己的zookeeper\n[想查看zookeeper安装的详细操作点这里！](https://blog.csdn.net/weixin_45366499/article/details/106782337)\n\n这样hbase-env.sh文件就配置成功了\n### （2）修改hbase-site.xml配置文件\n首先进入到/opt/Hadoop/hbase目录下\n\n```bash\nmkdir zookeeper-data\n```\n创建zookeeper-data目录文件\n\n```bash\nvim hbase-site.xml\n```\n添加如下内容\n\n```bash\n<configuration>\n         <!-- Hbase的数据保存在HDFS对应的目录下 -->\n        <property>\n                <name>hbase.rootdir</name>\n                <value>hdfs://node1:8020/hbase</value>\n        </property>\n\n        <!-- 是否是分布式环境 -->\n        <property>\n                <name>hbase.cluster.distributed</name>\n                <value>true</value>\n        </property>\n\n        <!-- 配置ZK的地址，3个节点都启用Zookeeper -->\n        <property>\n                <name>hbase.zookeeper.quorum</name>\n                <value>node1,node2,node3</value>\n        </property>\n\n        <!-- 冗余度 -->\n        <property>\n                <name>dfs.replication</name>\n                <value>2</value>\n        </property>\n\n        <!-- 主节点和从节点允许的最大时间误差 -->\n        <property>\n                <name>hbase.master.maxclockskew</name>\n                <value>180000</value>\n        </property>\n\n        <!-- zookeeper数据目录 -->\n        <property>\n                <name>hbase.zookeeper.property.dataDir</name>\n                <value>/opt/Hadoop/hbase/zookeeper-data</value>\n        </property>\n\t\t<!-- 设置网页端口号 -->\n        <property>\n                <name>hbase.master.info.port</name>\n                <value>60010</value>\n        </property>\n</configuration>\n```\n根据自己的配置适当修改\n\n### （3）修改regionservers 文件\n\n```bash\ncd /opt/Hadoop/hbase/conf\nvim regionservers\n```\n将下面的内容换成\n\n```bash\nnode1\nnode2\nnode3\n```\n### （4）配置backup-masters （可选）\n为了增加hbase集群的可用性，可以为hbase增加多个backup master。当master挂掉后，backup master可以自动接管整个hbase的集群。配置backup master的方式是在hbase的conf下增加文件backup-masters，在该文件里面增加backup master的机器列表，每台机器一条记录。\n\n```bash\ncd /opt/Hadoop/hbase/conf\ntouch backup-masters\n```\n在里面添加内容\n\n```bash\nnode2\n```\n## 第四步：分发配置文件\n将node1的hbase-1.2.0-cdh5.9.3和hbase-1.2.0-cdh5.9.3.tar.gz文件分发到node2和node3上\n\n```bash\nscp -r hbase-1.2.0-cdh5.9.3 hbase-1.2.0-cdh5.9.3.tar.gz caizhengjie@node2:/opt/Hadoop/\nscp -r hbase-1.2.0-cdh5.9.3 hbase-1.2.0-cdh5.9.3.tar.gz caizhengjie@node3:/opt/Hadoop/\n```\n分别在node2和node3上创建软连接\n\n```bash\nln -s hbase-1.2.0-cdh5.9.3 hbase\n```\n## 第五步：启动与检测\n在启动HBase之前要先把三台机的zookeeper给启动起来，不然会出现刚启动hbase的进程就消失，我通过查看日志文件，找出的原因是zookeeper没有启动。报错信息如下：\n\n```bash\n020-06-20 11:31:30,550 INFO  [main-SendThread(node1:2181)] zookeeper.ClientCnxn: Opening socket connection to server node1/10.211.55.59:2181. Will not attempt to authenticate using SASL (unknown error)\n2020-06-20 11:31:30,550 WARN  [main-SendThread(node1:2181)] zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect\njava.net.ConnectException: 拒绝连接\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)\n\tat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)\n2020-06-20 11:31:30,652 INFO  [main-SendThread(node2:2181)] zookeeper.ClientCnxn: Opening socket connection to server node2/10.211.55.60:2181. Will not attempt to authenticate using SASL (unknown error)\n2020-06-20 11:31:30,652 ERROR [main] zookeeper.RecoverableZooKeeper: ZooKeeper create failed after 4 attempts\n2020-06-20 11:31:30,652 ERROR [main] master.HMasterCommandLine: Master exiting\njava.lang.RuntimeException: Failed construction of Master: class org.apache.hadoop.hbase.master.HMaster. \n\tat org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:2486)\n\tat org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:231)\n\tat org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:137)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:127)\n\tat org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:2496)\nCaused by: org.apache.hadoop.hbase.ZooKeeperConnectionException: master:600000x0, quorum=node1:2181,node2:2181,node3:2181, baseZNode=/hbase Unexpected KeeperException creating base node\n\tat org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.createBaseZNodes(ZooKeeperWatcher.java:206)\n\tat org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:187)\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:594)\n\tat org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:420)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:2479)\n\t... 5 more\nCaused by: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:99)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.createNonSequential(RecoverableZooKeeper.java:565)\n\tat org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.create(RecoverableZooKeeper.java:544)\n\tat org.apache.hadoop.hbase.zookeeper.ZKUtil.createWithParents(ZKUtil.java:1204)\n\tat org.apache.hadoop.hbase.zookeeper.ZKUtil.createWithParents(ZKUtil.java:1182)\n\tat org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.createBaseZNodes(ZooKeeperWatcher.java:194)\n\t... 13 more\n```\n下面开始启动HBase，在node1上输入命令：\n\n```bash\nstart-hbase.sh\n```\n查看jps\nnode1会出现：\n\n```bash\n[caizhengjie@node1 ~]$ jps\n2769 SecondaryNameNode\n4804 QuorumPeerMain\n4984 HMaster\n6202 Jps\n2475 NameNode\n5244 HRegionServer\n2606 DataNode\n```\nnode2会出现：\n\n```bash\n[caizhengjie@node2 Hadoop]$ jps\n3427 HRegionServer\n2215 DataNode\n3659 HMaster\n3340 QuorumPeerMain\n4302 Jps\n```\nnode3会出现：\n\n```bash\n[caizhengjie@node3 logs]$ jps\n2784 QuorumPeerMain\n3431 Jps\n2027 DataNode\n2878 HRegionServer\n```\n则表示HBase安装成功，这里有的人会发现我的node1和node2出现了两个HMaster，因为我设置了backup-masters。\n\n下面我来访问一下网页：[http://10.211.55.59:60010/master-status](http://10.211.55.59:60010/master-status)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620124248608.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620124248604.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n一切正常！\n## 第六步：常见问题\n在启动HBase的过程中会出现一些问题，不过也不要被这些问题吓到，通过查看日志文件，都是可以找到解决方案的。\n**我遇到的第一个问题：No space left on device**\n意思是磁盘空间不够，之前我安装的Hadoop，Hbase都是安装在/home/caizhengjie/目录下面的，但是随着文件数量新增，出现了磁盘空间不够，我通过`df -h`命令查看磁盘空间，果然不够\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200620125002744.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n由图片可以看出home目录下的磁盘空间只有2.6G，而根目录下的空间有27G，因为是虚拟机，所以我可以在后面继续扩磁盘空间。因此我只好把Hadoop那些文件全部转移到/opt/目录下，这下就解决了磁盘不足的问题。\n\n**我遇到的第二个问题：运行HBase报SLF4J: Class path contains multiple SLF4J bindings**\n主要原因是slf4j-log4j12.jar包和Hadoop中的slf4j-log4j12.jar包冲突导致的\n解决方法是将/opt/Hadoop/hbase/lib中的slf4j-log4j12.jar包删除即可，但是不要将三台机的slf4j-log4j12.jar包都给删掉，不然又会报错：**Failed to load class org.slf4j.impl.StaticLoggerBinder**，我是把node1和node2的给删除掉了，留下node3\n\n参考文章：\n\n - 解决slf4j问题\n - [http://www.slf4j.org/codes.html](http://www.slf4j.org/codes.html)\n - 解决Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0问题\n - [https://www.cnblogs.com/QuestionsZhang/p/10281839.html](https://www.cnblogs.com/QuestionsZhang/p/10281839.html)\n - 解决SLF4J: Class path contains multiple SLF4J bindings问题\n - [https://blog.csdn.net/qq_27575895/article/details/90238240](https://blog.csdn.net/qq_27575895/article/details/90238240)\n - 解决HBase安装问题\n - [https://www.jianshu.com/p/ecae88481db2](https://www.jianshu.com/p/ecae88481db2)\n - 解决Linux磁盘空间不足的问题\n - [https://blog.csdn.net/u010455714/article/details/77711834](https://blog.csdn.net/u010455714/article/details/77711834)\n\n## 总结\n对于新手安装HBase来说，就是照着其他的模版来也未必能不踩坑的顺利安装好，这里我就总结两点经验。第一：坚持！一定要坚持！行百里者半九十。第二：一定要多看日志文件！这一点非常重要，因为并不是所有的报错信息在网上都能找到，只有看到报错信息才能知道哪里的问题。通常对于小白来说，什么是日志信息？日志信息说白了就是所有的报错信都在日志信息里面。\n如何查看日志信息？通常来说可以使用more或cat命令，日志信息在/opt/Hadoop/hbase/logs目录下\n可以通过下面的命令来查看日志信息\n```bash\ncat hbase-caizhengjie-master-node1.log \n```\n","tags":["Hadoop"],"categories":["Hadoop学习指南"]},{"title":"Centos下ZooKeeper安装部署配置（集群模式）","url":"/posts/4170185227/","content":"## 第一步：准备文件\n<!-- more -->\n### （1）上传文件\n将zookeeper压缩文件上传至node1中，Mac系统上传方式可以直接通过终端scp命令，Windows系统可以通过其他的上传工具。上传方式为：\n\n```bash\nscp /自己电脑本机路径/zookeeper-3.4.13.tar.gz caizhengjie@10.211.55.59:/opt/Hadoop\n```\n### （2）解压文件\n上传成功之后需要对文件赋予权限\n\n```bash\nchmod u+x zookeeper-3.4.13.tar.gz\n```\n解压文件：\n\n```bash\ntar -zxvf zookeeper-3.4.13.tar.gz\n```\n创建软链接：\n\n```bash\nln -s zookeeper-3.4.13 zookeeper\n```\n## 第二步：修改配置文件\n### （1）重命名文件\n在安装zookeeper的时候我们要去修改zookeeper预装是conf目录下面的zoo_sample.cfg这个文件，首先我们要做的事就是重命名这个文件。在目录/opt/Hadoop/zookeeper/conf下，将zoo_sample.cfg改名为zoo.cfg文件，这一步**非常重要**，不修改的话会出现下面的问题：\n\n```bash\nZooKeeper JMX enabled by default\nUsing config: /home/caizhengjie/zookeeper/bin/../conf/zoo.cfg\ngrep: /home/caizhengjie/zookeeper/bin/../conf/zoo.cfg: 没有那个文件或目录\nmkdir: 无法创建目录\"\": 没有那个文件或目录\nStarting zookeeper ... /home/caizhengjie/zookeeper/bin/zkServer.sh:行149: /zookeeper_server.pid: 权限不够\nFAILED TO WRITE PID\n```\n这是第一个坑！\n则我们需要修改文件名：\n```bash\nmv zoo_sample.cfg  zoo.cfg\n```\n\n### （2）创建tmp文件夹\n\n```bash\ncd /opt/Hadoop/zookeeper/\n```\n```bash\nmkdir tmp\n```\n```bash\ncd tmp\n```\n```bash\nmkdir data\n```\n### （3）创建myid文件\n```bash\ncd /opt/Hadoop/zookeeper/tmp/data\nvim myid\n```\n第一台主机node1添加内容：1\n**注意：一定要在刚才创建的data文件夹下在创建myid**\n如果直接在tmp文件夹下直接创建myid文件，查看zookeeper.out日志文件会报错\n\n```bash\nCaused by: java.lang.IllegalArgumentException: /opt/Hadoop/zookeeper/tmp/data/myid file is missing\n```\n这是第二个坑！\n\n### （4）修改配置文件\n修改zookeeper/conf下zoo.cfg文件\n\n```bash\nvim zoo.cfg\n```\n\n```bash\n#The number of milliseconds of each tick\ntickTime=2000\n#The number of ticks that the initial \n#synchronization phase can take\ninitLimit=5\n#The number of ticks that can pass between \n#sending a request and getting an acknowledgement\nsyncLimit=2\n#the directory where the snapshot is stored.\n#do not use /tmp for storage, /tmp here is just \n#example sakes.\n#**这个地方填写自己的路径**\ndataDir=/opt/Hadoop/zookeeper/tmp/data\n#the port at which the clients will connect\nclientPort=2181\n#the maximum number of client connections.\n#increase this if you need to handle more clients\n#maxClientCnxns=60 \n#服务器名称与地址:集群信息(服务器编号、服务器编号、服务器地址、LF通信端口、选举端口)\nserver.1=node1:2888:3888\nserver.2=node2:2888:3888\nserver.3=node3:2888:3888\n```\n## 第三步：配置环境变量\n配置环境变量：\n```bash\nvim ~/.bashrc\n```\n添加下面内容：\n```bash\nexport ZOOKEEPER_HOME=/opt/Hadoop/zookeeper\nexport PATH=$ZOOKEEPER_HOME/bin:$PATH\n```\n这里需要根据自己安装的路径来填写\n使之生效： \n\n```bash\nsource ~/.bashrc\n```\n## 第四步：分发文件\n### （1）分发文件\n在node1配置好文件之后，需要将文件分发到node2，node3机器下面。\n\n```bash\nscp -r zookeeper zookeeper-3.4.13 zookeeper-3.4.13.tar.gz caizhengjie@node2:/opt/Hadoop/\n```\n\n```bash\nscp -r zookeeper zookeeper-3.4.13 zookeeper-3.4.13.tar.gz caizhengjie@node3:/opt/Hadoop/\n```\n### （2）2、3机配置环境变量\n参考node1的配置方法\n### （3）修改myid文件\n前面在/opt/Hadoop/zookeeper/tmp/data/myid文件中，第一台主机添加内容：1\n则在node2和node3中分别按下面修改\n第二台主机添加内容：2\n第三台主机添加内容：3\n## 第五步：启动与查看运行状态\n按照上面的步骤全部配置完成之后，可以启动zookeeper\n启动命令(三台机同时启动)：\n\n```bash\nzkServer.sh start\n```\n关闭命令：\n\n```bash\nzkServer.sh stop\n```\n出现下面的情况则安装成功\n```bash\nZooKeeper JMX enabled by default\nUsing config: /opt/Hadoop/zookeeper/bin/../conf/zoo.cfg\nStarting zookeeper ... STARTED\n```\n检验jps进程\n```bash\n4616 Jps\n2041 QuorumPeerMain\n```\n查看运行状态（三台机同时）\n\n```bash\nzkServer.sh status\n```\nnode1\n```bash\nZooKeeper JMX enabled by default\nUsing config: /opt/Hadoop/zookeeper/bin/../conf/zoo.cfg\nMode: follower\n```\nnode2\n```bash\nZooKeeper JMX enabled by default\nUsing config: /opt/Hadoop/zookeeper/bin/../conf/zoo.cfg\nMode: leader\n```\nnode3\n```bash\nZooKeeper JMX enabled by default\nUsing config: /opt/Hadoop/zookeeper/bin/../conf/zoo.cfg\nMode: follower\n```\n会发现其中有一台机器是leader，其他两台机器是follower\n到这里zookeeper集群模式就安装成功了！\n\n总结：如果运行出错，多看zookeeper.out日志文件\n","tags":["Hadoop"],"categories":["Hadoop学习指南"]},{"title":"Hadoop常用命令","url":"/posts/2985237682/","content":"## 一、前述\n### （1）启动Hadoop所有进程\n**start-all.sh等价于start-dfs.sh + start-yarn.sh**\n<!-- more -->\n但是一般不推荐使用start-all.sh(因为开源框架中内部命令启动有很多问题)。\n### （2）单进程启动\n启动HDFS：\n```bash\nsbin/start-dfs.sh\n```\n```bash\n\tsbin/hadoop-daemons.sh --config .. --hostname .. start namenode ...\n    sbin/hadoop-daemons.sh --config .. --hostname .. start datanode ...\n    sbin/hadoop-daemons.sh --config .. --hostname .. start sescondarynamenode ...\n    sbin/hadoop-daemons.sh --config .. --hostname .. start zkfc ...         //\n```\n启动YARN：\n\n```bash\nsbin/start-yarn.sh\n```\n\n```bash\n\tlibexec/yarn-config.sh\n    sbin/yarn-daemon.sh --config $YARN_CONF_DIR  start resourcemanager\n    sbin/yarn-daemons.sh  --config $YARN_CONF_DIR  start nodemanager\n```\n### （3）关闭Hadoop所有进程\n关闭Hadoop服务：\n\n```bash\nstop-all.sh\n```\n\n### （4）格式化\n\n```bash\nhdfs namenode -format\n```\n\n## 二、常用命令\n### （1）查看指定目录下内容\nhdfs dfs –ls [文件目录]\n\n```bash\nhdfs dfs -ls -R  [文件目录]        //显式目录结构\n```\n\n```bash\nhdfs dfs -ls -d  [文件目录]          //返回的是path\n```\n\n```bash\nhdfs dfs -ls -h  [文件目录]          //h指的是“human-readable”，按照人性化的单位显示文件大小\n```\n\neg：hdfs dfs –ls /user/wangkai.pt\n### （2）查看某个文件\n\n```bash\nhdfs dfs –cat [file_path]\n```\n eg:hdfs dfs -cat /user/wangkai.pt/data.txt\n### （3）创建文件夹\n\n```bash\nhdfs dfs -mkdir [文件夹名称]\t\t\t//父目录存在的情况下\n```\n```bash\nhdfs dfs -mkdir -p [文件夹名称]\t   //父目录不存在（首先会创建父目录）\n```\n### （4）新建文件\n\n```bash\nhdfs dfs -touchz <paths>\n```\n### （5）将本地文件夹存储至HDFS\n\n```bash\nhdfs dfs -put [-f] [-p] [本地目录] [hdfs目录] \n```\n\n```bash\nhdfs dfs -copyFromLocal [-f] [-p] [-l] [本地目录] [hdfs目录] \n```\nput 或 copyFromLocal命令是将本地文件上传到HDFS。\n### （6）将本地文件移动到HDFS\n\n```bash\nhdfs dfs -moveFromLocal [本地目录] [hdfs目录]\n```\n使用这个命令，本地文件会被删除，移到了hdfs上\n\n### （7）下载文件\n\n```bash\nhdfs dfs -get [-p] [hdfs文件目录] [本地目录]\n```\n\n```bash\nhdfs dfs -copyToLocal [-p] [-ignoreCrc] [-crc] [hdfs文件目录] [本地目录]\n```\nget 或 copyToLocal 命令把文件从分布式系统复制到本地\n### （8）删除hadoop上指定文件或目录\n```bash\nhdfs  dfs –rm [hdfs文件地址]\n```\neg：hdfs dfs –rm /user/t/ok.txt\n\n```bash\nhdfs dfs -rm [-f] [-r] [hdfs文件目录]\n```\n-f：如果要删除的文件不存在，不显示错误信息。\n-r/R：级联删除目录下的所有文件和子目录文件。\n### （9）将hadoop上某个文件重命名\n使用mv命令：\n\n```bash\n hdfs dfs –mv  /user/test.txt  /user/ok.txt   （将test.txt重命名为ok.txt）\n```\n### （10）显示占用的磁盘空间大小\n\n```bash\nhdfs dfs -du [-s] [-h] <path>\n```\n按字节显示目录所占空间的大小。-s指的是显示指定目录下的文件总的大小，-h指的是“human-readable”，按照人性化的单位显示文件大小。\n### （11）HDFS中的文件复制\n\n```bash\nhdfs dfs -cp [-f] [-p | -p[topax]] [本地目录] [hdfs目录]\n```\n-f：如果目录文件存在，将强行覆盖。\n-p：将保存文件的属性。\n\n### （12）统计\n```bash\nhdfs dfs -count [-q] [-h] <path>\n```\n统计某个目录下的子目录与文件的个数及大小。统计结果包含目录数、文件数、文件大小。\n\n### （13）HDFS中的文件合并后下载到本地\n```bash\nhdfs dfs -getmerge [-nl] [hdfs文件目录] [本地目录]\n```\n### （14）将正在运行的hadoop作业kill掉\n\n```bash\n hadoop job –kill  [job-id]\n```\n### （15）安全模式\n安全模式(Safemode)是HDFS所处的一种特殊状态。处于这种状态时，HDFS只接受读数据请求，不能对文件进行写、删除等操作。\na 查看当前状态\n\n```bash\nhdfs dfsadmin -safemode get\n```\nb 进入安全模式\n\n```bash\nhdfs dfsadmin -safemode enter\n```\nc 强制离开安全模式\n\n```bash\nhdfs dfsadmin -safemode leave\n```\nd 一直等待，知道安全模式结束\n\n```bash\nhdfs dfsadmin -safemode wait\n```\n### （16）查看帮助\n\n```bash\nhdfs dfs -help \n```\n### （17）设置扩展属性\n\n```bash\nhdfs dfs -setfattr {-n name [-v value] | -x name} <path>\n```\n其中，采用hdfs dfs -setfattr -n name [-v value] | -x name <path> 可以设置属性。\n采用hdfs dfs -setfattr -x name <path>可以删除属性。\n-n：指定属性名称（设置属性时用）\n-v：指定属性值\n-x：指定属性的名称（删除属性时用）\n### （18）获取扩展属性\n\n```bash\nhdfs dfs -getfattr [-R] {-n name [-v value] | -x name} <path>\n```\n-n：指定属性名称\n-d：指定dump，即显示所有属性\n-e：指encoding，包含text、hex、base64等。\n### （19）HDFS管理命令\n**报告文件系统的基本信息和统计信息**\n```bash\nhdfs dfsadmin -report\n```\n**查看拓扑**\n```bash\nhdfs dfsadmin -printTopology\n```\n\n以上命令基本上为Hadoop的常用命令，除此之外均为其他命令。\n","tags":["Hadoop"],"categories":["Hadoop学习指南"]},{"title":"Java集合详解","url":"/posts/1169821274/","content":"## 一、集合简介\n集合本质是基于某种数据结构数据容器。常见的数据结构:数组(Array)、集(Set)、队列 (Queue)、链表(Linkedlist)、树(Tree)、堆(Heap)、栈(Stack)和映射(Map)等结构。\n<!-- more --> \nJava中提供了丰富的集合接口和类，它们来自于java.util包。如图所示是Java主要的集合接口和 类，从图中可见Java集合类型分为:Collection和Map，Collection子接口有:Set、Queue和List等接口。 每一种集合接口描述了一种数据结构。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20200210174542205.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM2NjQ5OQ==,size_16,color_FFFFFF,t_70)\n在Java SE中List名称的类型有两个，一个是java.util.List，另外一个是java.awt.List。 java.util.List是一个接口，而java.awt.List是一个类，用于图形用户界面开 发，它是一个图形界面中的组件。学习Java中的集合，首先从两大接口入手，重点掌握List、Set和Map三个接口，熟悉这些接 口中提供的方法。然后再熟悉这些接口的实现类，并了解不同实现类之间的区别。\n\n## 二、List集合\nList集合中的元素是有序的，可以重复出现。List接口的实现类有:ArrayList 和 LinkedList。ArrayList是基于动态数组数据结构的实现，LinkedList 是基于链表数据结构的实现。ArrayList访问元素速度优于LinkedList，LinkedList占用的内存空间比较 大，但LinkedList在批量插入或删除数据时优于ArrayList。\n### （1）常用方法\nList接口继承自Collection接口，List接口中的很多方法都继承自Collection接口的。List接口中常用方法如下。\n01. 操作元素\nget(int index):返回List集合中指定位置的元素。\nset(int index, Object element):用指定元素替换List集合中指定位置的元素。\nadd(Object element):在List集合的尾部添加指定的元素。该方法是从Collection集合继承 过来的。\nadd(int index, Object element):在List集合的指定位置插入指定元素。 remove(int index):移除List集合中指定位置的元素。\nremove(Object element):如果List集合中存在指定元素，则从List集合中移除第一次出现的 指定元素。该方法是从Collection集合继承过来的。\nclear():从List集合中移除所有元素。该方法是从Collection集合继承过来的。\n 02. 判断元素\nisEmpty():判断List集合中是否有元素，没有返回true，有返回false。该方法是从 Collection集合继承过来的。\ncontains(Object element):判断List集合中是否包含指定元素，包含返回true，不包含返回 false。该方法是从Collection集合继承过来的。\n03. 查询元素\nindexOf(Object o):从前往后查找List集合元素，返回第一次出现指定元素的索引，如果\n此列表不包含该元素，则返回-1。\nlastIndexOf(Object o):从后往前查找List集合元素，返回第一次出现指定元素的索引，如果此列表不包含该元素，则返回-1。\n04. 其他\niterator():返回迭代器(Iterator)对象，迭代器对象用于遍历集合。该方法是从Collection 集合继承过来的。\nsize():返回List集合中的元素数，返回值是int类型。该方法是从Collection集合继承过来 的。\nsubList(int fromIndex, int toIndex):返回List集合中指定的 fromIndex(包括 )和 toIndex(不包括)之间的元素集合，返回值为List集合。\n代码如下：\n\n```java\npackage 集合.list集合;\nimport\tjava.util.ArrayList;\nimport java.util.List;\n//list集合：有序，重复\npublic class HelloWorld {\n    public static void main(String[] args) {\n//\n        List list = new ArrayList();\n        String b = \"B\";\n//       向集合中添加元素\n        list.add(\"A\");\n        list.add(b);\n        list.add(\"C\");\n        list.add(b);\n        list.add(\"D\");\n        list.add(\"E\");\n\n//        打印集合的元素个数\n        System.out.println(\"集合 size = \"+list.size());\n//        打印集合\n        System.out.println(list);\n//从前往后查找b元素\n        System.out.println(\"indexOf(\\\"B\\\") = \" +list.indexOf(b));\n//       从后往前查找\"B\"元素\n        System.out.println(\"lastindexOf(\\\"B\\\") = \" +list.lastIndexOf(b));\n        //删除集合中第一个\"B\"元素\n        list.remove(b);\n        System.out.println(\"remove(3)前: \"+list);\n        //判断集合中是否包含\"B\"元素\n        System.out.println(\"是否包含\\\"B\\\":\" + list.contains(b));\n\n        //删除集合第4个元素\n        list.remove(3);\n        System.out.println(\"remove(3)后: \" + list);\n\n        //判断集合是否为空\n        System.out.println(\"list集合是空的:\" + list.isEmpty());\n\n        System.out.println(\"替换前:\" + list); //替换集合第2个元素\n        list.set(1, \"F\");\n        System.out.println(\"替换后:\" + list);\n\n        //清空集合\n        list.clear();\n        System.out.println(list);\n\n        // 重新添加元素\n        list.add(1);// 发生自动装箱\n        list.add(3);\n        \n        int item = (Integer)list.get(0);//发生自动拆箱\n    }\n}\n```\n运行结果：\n\n```java\n集合 size = 6\n[A, B, C, B, D, E]\nindexOf(\"B\") = 1\nlastindexOf(\"B\") = 3\nremove(3)前: [A, C, B, D, E]\n是否包含\"B\":true\nremove(3)后: [A, C, B, E]\nlist集合是空的:false\n替换前:[A, C, B, E]\n替换后:[A, F, B, E]\n[]\n```\n### （2）遍历集合\n集合最常用的操作之一是遍历，遍历就是将集合中的每一个元素取出来，进行操作或计算。List集合遍历有三种方法:\n01. 使用for循环遍历:List集合可以使用for循环进行遍历，for循环中有循环变量，通过循环变量可\n以访问List集合中的元素。\n02. 使用for-each循环遍历:for-each循环是针对遍历各种类型集合而推出的，笔者推荐使用这种遍历\n方法。\n03. 使用迭代器遍历:Java提供了多种迭代器，List集合可以使用Iterator和ListIterator迭代器。\n代码如下：\n\n```java\npackage 集合.list集合遍历;\n\nimport java.util.ArrayList;\nimport java.util.Iterator;\nimport java.util.List;\n\npublic class HelloWorld {\n    public static void main(String[] args) {\n        List list = new ArrayList();\n\n        String b = \"B\";\n//       向集合中添加元素\n        list.add(\"A\");\n        list.add(b);\n        list.add(\"C\");\n        list.add(b);\n        list.add(\"D\");\n        list.add(\"E\");\n\n//        打印集合\n        System.out.println(list);\n//        for循环遍历集合\n        System.out.println(\"--1.使用for循环遍历--\");\n        for (int i = 0;i<list.size();i++){\n//            System.out.println(list.get(i));\n            System.out.printf(\"读取集合元素(%d): %s \\n\", i, list.get(i));\n        }\n        // 2.使用for-each循环遍历\n        System.out.println(\"--2.使用for-each循环遍历--\");\n        for (Object items:list){\n            String s = (String) items;\n            System.out.println(\"读取集合元素：\"+s);\n        }\n\n        // 3.使用迭代器遍历\n        System.out.println(\"--3.使用迭代器遍历--\");\n        Iterator iterator = list.iterator();\n        while (iterator.hasNext()){\n            Object items = iterator.next();\n            String s = (String)items;\n            System.out.println(\"读取集合元素：\"+s);\n        }\n    }\n}\n```\n运行结果：\n\n```java\n[A, B, C, B, D, E]\n--1.使用for循环遍历--\n读取集合元素(0): A \n读取集合元素(1): B \n读取集合元素(2): C \n读取集合元素(3): B \n读取集合元素(4): D \n读取集合元素(5): E \n--2.使用for-each循环遍历--\n读取集合元素：A\n读取集合元素：B\n读取集合元素：C\n读取集合元素：B\n读取集合元素：D\n读取集合元素：E\n--3.使用迭代器遍历--\n读取集合元素：A\n读取集合元素：B\n读取集合元素：C\n读取集合元素：B\n读取集合元素：D\n读取集合元素：E\n```\n## 二、Set集合\nSet集合是由一串无序的，不能重复的相同类型元素构成的集合。List集合强调的是有序，Set集合强调的是不重复。当不考虑顺序，且没有重复元素时，Set集合和List集 合可以互相替换的。Set接口直接实现类主要是HashSet，HashSet是基于散列表数据结构的实现。\n### （1）常用方法\nSet接口也继承自Collection接口，Set接口中大部分都是继承自Collection接口，这些方法如下。\n01. 操作元素\nadd(Object element):在Set集合的尾部添加指定的元素。该方法是从Collection集合继承过来的。\nremove(Object element):如果Set集合中存在指定元素，则从Set集合中移除该元素。该方法是从Collection集合继承过来的。\nclear():从Set集合中移除所有元素。该方法是从Collection集合继承过来的。 \n02. 判断元素\nisEmpty():判断Set集合中是否有元素，没有返回true，有返回false。该方法是从 Collection集合继承过来的。\ncontains(Object element):判断Set集合中是否包含指定元素，包含返回true，不包含返回 false。该方法是从Collection集合继承过来的。\n03. 其他\niterator():返回迭代器(Iterator)对象，迭代器对象用于遍历集合。该方法是从Collection\n集合继承过来的。\nsize():返回Set集合中的元素数，返回值是int类型。该方法是从Collection集合继承过来的。\n代码如下：\n\n```java\n//set集合：无序，不重复\nimport java.util.HashSet;\n\npublic class Set {\n    public static void main(String[] args) {\n        java.util.Set set = new HashSet();\n\n        String b = \"B\";\n// 向集合中添加元素\n        set.add(\"A\");\n        set.add(b);\n        set.add(\"C\");\n        set.add(b);\n        set.add(\"D\");\n        set.add(\"E\");\n//        打印集合个数\n        System.out.println(\"集合size = \"+set.size());\n//        打印集合\n        System.out.println(set);\n//       删除集合中的B元素\n        set.remove(b);\n//        判断集合中是否包含\"B\"元素\n        System.out.println(\"判断是否包含B元素\"+set.contains(b));\n//        判断集合是否为空\n        System.out.println(\"判断集合是否为空\"+set.isEmpty());\n//        清空集合\n        set.clear();\n        //        打印集合\n        System.out.println(set);\n    }\n}\n```\n运行结果：\n\n```java\n集合size = 5\n[A, B, C, D, E]\n判断是否包含B元素false\n判断集合是否为空false\n[]\n```\n### （2）遍历集合\nSet集合中的元素由于没有序号，所以不能使用for循环进行遍历，但可以使用for-each循环和迭代器进 行遍历。事实上这两种遍历方法也是继承自Collection集合，也就是说所有的Collection集合类型都有这 两种遍历方式。\n代码如下：\n\n```java\npublic class Bianli {\n    public static void main(String[] args) {\n        Set set = new HashSet();\n        String b = \"B\";\n// 向集合中添加元素\n        set.add(\"A\");\n        set.add(b);\n        set.add(\"C\");\n        set.add(b);\n        set.add(\"D\");\n        set.add(\"E\");\n\n        //        打印集合\n        System.out.println(set);\n//        1，使用增强for遍历\n        System.out.println(\"--1，使用增强for遍历--\");\n        for (Object item : set){\n            String s = (String)item;\n            System.out.println(s);\n        }\n//        2.使用迭代器遍历集合\n        System.out.println(\"--2.使用迭代器遍历集合--\");\n        Iterator iterator = set.iterator();\n        while (iterator.hasNext()){\n            Object item  = iterator.next();\n            String s = (String)item;\n            System.out.println(\"读取集合\"+s);\n        }\n\n    }\n}\n```\n运行结果：\n\n```java\n[A, B, C, D, E]\n--1，使用增强for遍历--\nA\nB\nC\nD\nE\n--2.使用迭代器遍历集合--\n读取集合A\n读取集合B\n读取集合C\n读取集合D\n读取集合E\n```\n## 三、Map集合\nMap(映射)集合表示一种非常复杂的集合，允许按照某个键来访问元素。Map集合是由两个集合构 成的，一个是键(key)集合，一个是值(value)集合。键集合是Set类型，因此不能有重复的元素。 而值集合是Collection类型，可以有重复的元素。Map集合中的键和值是成对出现的。Map接口直接实现类主要是HashMap，HashMap是基于散列表数据结构的实现。\n### （1）常用方法\nMap集合中包含两个集合(键和值)，所以操作起来比较麻烦，Map接口提供很多方法用来管理和操 作集合。主要的方法如下。\n01. 操作元素\nget(Object key):返回指定键所对应的值;如果Map集合中不包含该键值对，则返回null。 put(Object key, Object value):指定键值对添加到集合中。\nremove(Object key):移除键值对。\nclear():移除Map集合中所有键值对。 \n02. 判断元素\nisEmpty():判断Map集合中是否有键值对，没有返回true，有返回false。 \ncontainsKey(Object key):判断键集合中是否包含指定元素，包含返回true，不包含返回false。\ncontainsValue(Object value):判断值集合中是否包含指定元素，包含返回true，不包含返回false。\n03. 查看集合\nkeySet():返回Map中的所有键集合，返回值是Set类型。 \nvalues():返回Map中的所有值集合，返回值是Collection类型。 \nsize():返回Map集合中键值对数。\n代码如下：\n\n```java\npackage 集合.map集合;\n\nimport java.util.HashMap;\n\npublic class Map {\n    public static void main(String[] args) {\n        java.util.Map map = new HashMap();\n\n        map.put(101,\"A\");\n        map.put(102, \"B\");\n        map.put(103, \"C\");\n        map.put(104, \"D\");\n//        B重复\n        map.put(105, \"B\");\n//把102的值换成E\n        map.put(102, \"E\");\n//        打印集合\n        System.out.println(map);\n//        打印集合元素个数\n        System.out.println(\"集合size=\"+map.size());\n//        通过键取值\n        System.out.println(\"102-\"+map.get(102));\n        System.out.println(\"105-\"+map.get(105));\n//        删除键值对\n        map.remove(102);\n        System.out.println(map);\n//        判断集合中是否包含105\n        System.out.println(\"集合中是否包含102\"+map.containsKey(105));\n//        集合中是否包含\"A\"\n        System.out.println(\"集合中是否包含A\"+map.containsValue(\"A\"));\n//        判断集合是否为空\n        System.out.println(\"集合是否为空\"+map.isEmpty());\n//        清空集合\n        map.clear();\n        System.out.println(map);\n    }\n}\n```\n运行结果：\n\n```java\n{101=A, 102=E, 103=C, 104=D, 105=B}\n集合size=5\n102-E\n105-B\n{101=A, 103=C, 104=D, 105=B}\n集合中是否包含102true\n集合中是否包含Atrue\n集合是否为空false\n{}\n```\n### （2）遍历集合\nMap集合遍历与List和Set集合不同，Map有两个集合，因此遍历时可以只遍历值的集合，也可以只遍历键的集合，也可以同时遍历。这些遍历过程都可以使用for-each循环和迭代器进行遍历。\n代码如下：\n\n```java\npackage 集合.map集合遍历;\n\nimport java.util.*;\n\npublic class Mapbianli {\n    public static void main(String[] args) {\n        Map map = new HashMap();\n        map.put(101,\"A\");\n        map.put(102, \"B\");\n        map.put(103, \"C\");\n        map.put(104, \"D\");\n//        使用增强for循环遍历\n        System.out.println(\"使用增强for循环遍历\");\n//        获得键集合\n        Set keys  = map.keySet();\n        for (Object key:keys){\n//            自动拆箱\n            int ikey = (Integer) key;\n//            自动装箱\n            String value = (String)map.get(ikey);\n            System.out.printf(\"key=%d-value=%s \\n\",ikey,value);\n        }\n//        使用迭代器遍历集合\n        System.out.println(\"使用迭代器遍历集合\");\n//        获得值集合\n        Collection values = map.values();\n//        遍历值集合\n        Iterator iterator = values.iterator();\n        while (iterator.hasNext()){\n            Object item = iterator.next();\n            String s = (String)item;\n            System.out.println(\"集合元素集合:\"+s);\n        }\n    }\n}\n```\n运行结果：\n\n```java\n使用增强for循环遍历\nkey=101-value=A \nkey=102-value=B \nkey=103-value=C \nkey=104-value=D \n使用迭代器遍历集合\n集合元素集合:A\n集合元素集合:B\n集合元素集合:C\n集合元素集合:D\n```","tags":["Java"],"categories":["Java基础语法"]},{"title":"Java面试题实现线程的几种方式？","url":"/posts/3425737821/","content":"在Java面试中面试官常常会问这样一道面试题：Java面试题实现线程的几种方式？\n这道题看似简单也会难道很多人，下面总结一些实现线程的几种方式。\n<!-- more --> \n第一种：通过实现Runnable接口\n创建步骤：\n\n - 1、通过实现Runnable接口创建线程执行类 \n - 2、通过重写Runnable中的run方法，编写线程执行代码\n - 3、创建线程Thread对象，将线程执行对象传递给它 \n - 4、开始线程\n\n第二种：通过继承Thread线程类\n创建步骤：\n\n - 1、通过继承Thread线程类创建线程执行类 \n - 2、定义构造方法，通过super调用父类Thread构造方法 这两个Thread类\n   构造方法:\n   \n   Thread(String name):name为线程指定一个名字。 \n   Thread():线程名字是JVM分配的。\n   \n - 3、通过重写Thread中的run方法，编写线程执行代码 \n - 4、创建线程执行对象，将参数传递给它 \n - 5、开始线程\n\n示例代码：\n\n```java\nclass DogThread extends Thread{\n    @Override\n    public void run(){\n        System.out.println(\"dog eat\");\n    }\n}\n\nclass CatRunnable implements Runnable{\n    @Override\n    public void run() {\n        System.out.println(\"cat eat\");\n    }\n}\n\npublic class TestThread {\n    public static void main(String[] args) {\n        // 方案一：\n        DogThread dogThread = new DogThread();\n        dogThread.start();\n\n        // 方案二：\n        Thread thread = new Thread(new CatRunnable());\n        thread.start();\n\n//        使用内部类写法\n        Thread t1 = new Thread(new Runnable() {\n            @Override\n            public void run() {\n                System.out.println(\"cat eat\");\n            }\n        });\n        t1.start();\n\n//        使用lambda表达式写法\n        Thread t2 = new Thread(() ->{\n            System.out.println(\"cat eat\");\n        });\n        t2.start();\n    }\n}\n```\n运行结果：\n\n```java\ndog eat\ncat eat\ncat eat\ncat eat\n```\n关于线程的更多知识点见这篇文章[https://blog.csdn.net/weixin_45366499/article/details/104346644](https://blog.csdn.net/weixin_45366499/article/details/104346644)\n","tags":["Java"],"categories":["Java面试指南"]}]